{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4c2e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/potzschf/repos/')\n",
    "from helperToolz.helpsters import *\n",
    "from helperToolz.evapo import *\n",
    "import geopandas as gpd\n",
    "import tarfile\n",
    "from pyproj import CRS\n",
    "from collections import Counter\n",
    "from joblib import Parallel, delayed\n",
    "workhorse = True\n",
    "\n",
    "if workhorse:\n",
    "    origin = 'Aldhani/eoagritwin/'\n",
    "else:\n",
    "    origin = ''\n",
    "\n",
    "utm_to_epsg = {\n",
    "    '28': 32628,  # Western Portugal, Azores\n",
    "    '29': 32629,  # Western Spain, Portugal\n",
    "    '30': 32630,  # Spain, France, UK\n",
    "    '31': 32631,  # France, Benelux, Germany, Western Norway\n",
    "    '32': 32632,  # Germany, Denmark, Switzerland, Italy, Austria\n",
    "    '33': 32633,  # Central Europe: Poland, Czechia, Hungary, Croatia, Sweden, Norway\n",
    "    '34': 32634,  # Eastern Europe: Finland, Baltic States, Romania\n",
    "    '35': 32635,  # Western Russia, Ukraine\n",
    "    '36': 32636,  # Russia, Black Sea region\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13b941a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### get path and rows of scenes that have data for the chosen AOI (e.g. Brandenburg)\n",
    "aoi_set_man = 'Brandenburg'\n",
    "res = 30\n",
    "\n",
    "# load shapefiles and check projections\n",
    "ger = gpd.read_file(f'/data/{origin}misc/gadm41_DEU_shp/gadm41_DEU_1.shp')\n",
    "aoi = ger[ger['NAME_1'] == aoi_set_man]\n",
    "\n",
    "orbits = gpd.read_file(f'/data/{origin}misc/WRS2_descending_0/WRS2_descending.shp')\n",
    "\n",
    "if aoi.crs != orbits.crs:\n",
    "    orbits = orbits.to_crs(aoi.crs)\n",
    "\n",
    "# find overlapping paths/rows\n",
    "#intersecting = orbits[orbits.intersects(aoi.unary_union)]\n",
    "intersecting = gpd.sjoin(orbits, aoi, how=\"inner\", predicate=\"intersects\")\n",
    "path_rows = [[p, r] for p, r in zip(intersecting['PATH'], intersecting['ROW'])]\n",
    "# make sure, paths and rows have the correct format\n",
    "path_rows = [f'{str(p).zfill(3)}{str(r).zfill(3)}' for p, r in path_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9cc691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure same CRS\n",
    "orbits = orbits.to_crs(aoi.crs)\n",
    "\n",
    "# Find intersecting WRS-2 paths\n",
    "intersecting = gpd.sjoin(orbits, aoi, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot orbits that intersect\n",
    "intersecting.boundary.plot(ax=ax, color='blue', linewidth=1, label='Intersecting WRS-2 Paths')\n",
    "\n",
    "# Plot AOI\n",
    "aoi.boundary.plot(ax=ax, color='red', linewidth=2, label='AOI (Brandenburg)')\n",
    "\n",
    "# Style\n",
    "ax.set_title(\"Landsat WRS-2 Paths intersecting Brandenburg\", fontsize=14)\n",
    "ax.legend()\n",
    "ax.set_axis_off()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98ff14ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all paths from downloaded products --> subsetted to paths and rows\n",
    "landsat_files = getFilelist(f'/data/{origin}et/Landsat/raw/', '.tar.gz', deep=True)\n",
    "\n",
    "# create a look-up dictionary for time subsets\n",
    "lookUp = LandsatETFileManager(landsat_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "871dd620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1437808/114722487.py:17: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar.extractall(tempF)\n"
     ]
    }
   ],
   "source": [
    "#### do the compositing monthly\n",
    "year = 2024\n",
    "month = 7\n",
    "# check if temp_folder is empty and delete everything if not\n",
    "tempF = f'/data/{origin}et/Landsat/extracts/'\n",
    "if len(getFilelist(tempF, '.nc')) > 0:\n",
    "    for file in getFilelist(tempF, '.nc'):\n",
    "        os.remove(file)\n",
    "    print('kill complete')\n",
    "\n",
    "# subset data and extract\n",
    "year_month = lookUp.get_by_year_and_month(year, month)\n",
    "year_month_path_row = [scene for scene in year_month for pr in path_rows if pr in scene]\n",
    "\n",
    "for landsat_file in year_month_path_row:\n",
    "    with tarfile.open(landsat_file, 'r:gz') as tar:\n",
    "        tar.extractall(tempF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27088126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file paths (NetCDF or GeoTIFF)\n",
    "files_nc = getFilelist(f'/data/{origin}et/Landsat/extracts/', '.nc')\n",
    "files_xml = getFilelist(f'/data/{origin}et/Landsat/extracts/', 'T1.xml')\n",
    "datasets = []\n",
    "\n",
    "for f_nc in files_nc:\n",
    "    \n",
    "    landsat_epsg = utm_to_epsg[get_UTM_zone_from_xml(f_nc, files_xml)]\n",
    "    ds = xr.open_dataset(f_nc)\n",
    "    da = ds['ETA']\n",
    "    da.coords['XDim_ETA']\n",
    "    da.rio.set_spatial_dims(x_dim=\"XDim_ETA\", y_dim=\"YDim_ETA\", inplace=True)\n",
    "    da.rio.set_spatial_dims(x_dim=\"XDim_ETA\", y_dim=\"YDim_ETA\", inplace=True)\n",
    "    da.rio.write_crs(f'EPSG:{landsat_epsg}', inplace=True)  # or detect from file\n",
    "    datasets.append(da)\n",
    "\n",
    "# Use first scene as template\n",
    "template = datasets[0]\n",
    "\n",
    "# Align scenes\n",
    "aligned = [da.rio.reproject_match(template) for da in datasets]\n",
    "\n",
    "# Stack and composite\n",
    "stacked = xr.concat(aligned, dim=\"scene\")\n",
    "composite = stacked.median(dim=\"scene\", skipna=True)\n",
    "\n",
    "# Save result\n",
    "composite.rio.to_raster(f'/data/{origin}et/Landsat/composite.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b53eabd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kill complete\n"
     ]
    }
   ],
   "source": [
    "tempF = f'/data/{origin}et/Landsat/extracts/'\n",
    "if len(getFilelist(tempF, '.nc')) > 0:\n",
    "    for end in ['.nc', '.xml', '.txt']:\n",
    "        for file in getFilelist(tempF, end):\n",
    "            os.remove(file)\n",
    "    print('kill complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2fe520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get maximum bounding box based on aoi and nc data\n",
    "# nc_files = getFilelist(f'/data/{origin}et/Landsat/extracts/', '.nc')\n",
    "# # min-maxs for data\n",
    "# file_Xmin = []\n",
    "# file_Xmax = []\n",
    "# file_Ymin = []\n",
    "# file_Ymax = []\n",
    "\n",
    "# for nc_file in nc_files:\n",
    "#     file_Xmin.append(xr.open_dataset(nc_file).coords['XDim_ETA'][0])\n",
    "#     file_Xmax.append(xr.open_dataset(nc_file).coords['XDim_ETA'][-1])\n",
    "#     file_Ymin.append(xr.open_dataset(nc_file).coords['YDim_ETA'][-1])\n",
    "#     file_Ymax.append(xr.open_dataset(nc_file).coords['YDim_ETA'][0])\n",
    "\n",
    "# dXmin = min(file_Xmin) # western border\n",
    "# dXmax = max(file_Xmax) # eastern border\n",
    "# dYmin = min(file_Ymin) # southern border\n",
    "# dYmax = max(file_Ymax) # northern border\n",
    "\n",
    "# # min-max aoi\n",
    "# aXmin, aYmin, aXmax, aYmax = aoi.to_crs(CRS.from_epsg(landsat_epsg)).total_bounds\n",
    "\n",
    "# # get bounding box for array and later export\n",
    "# nXmin = int(math.floor((aXmin - dXmin) / res) * res + dXmin)\n",
    "# nXmax = int(dXmax - (math.floor((dXmax - aXmax) / res) * res))\n",
    "# nYmin = int(math.floor((aYmin - dYmin) / res) * res + dYmin)\n",
    "# nYmax = int(dYmax - (math.floor((dYmax - aYmax) / res) * res))\n",
    "\n",
    "# # create empty np array\n",
    "# rows = int(((nYmax - nYmin) / res) + 1)\n",
    "# cols = int(((nXmax - nXmin) / res) + 1)\n",
    "# bands = len(year_month_path_row)\n",
    "# block = np.empty((rows, cols, bands), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9147e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill it\n",
    "# for i, nc_file in enumerate(nc_files):\n",
    "#     print(i)\n",
    "#     values = xr.open_dataset(nc_file).data_vars['ETA'].values\n",
    "#     fXmin = int(file_Xmin[i])\n",
    "#     fXmax = int(file_Xmax[i])\n",
    "#     fYmin = int(file_Ymin[i])\n",
    "#     fYmax = int(file_Ymax[i])\n",
    "\n",
    "#     if nXmin - fXmin < 0: \n",
    "#         start_col_aoi = int((fXmin - nXmin) / 30)\n",
    "#         start_col_dat = 0\n",
    "#     elif nXmin - fXmin > 0:\n",
    "#         start_col_aoi = 0\n",
    "#         start_col_dat = int((nXmin - fXmin) / 30)\n",
    "#     else:\n",
    "#         start_col_aoi = 0\n",
    "#         start_col_dat = 0\n",
    "\n",
    "\n",
    "#     if nXmax - fXmax < 0:\n",
    "#         end_col_dat = values.shape[1] - int((fXmax-nXmax)/30)\n",
    "#         end_col_aoi = cols\n",
    "#     elif nXmax - fXmax > 0:\n",
    "#         end_col_dat = values.shape[1]\n",
    "#         end_col_aoi = cols - int((nXmax - fXmax) / 30)\n",
    "#     else:\n",
    "#         end_col_dat = values.shape[1]\n",
    "#         end_col_aoi = cols\n",
    "\n",
    "\n",
    "#     if nYmax - fXmax < 0:\n",
    "#         start_row_dat = int((fYmax - nYmax) / 30)\n",
    "#         start_row_aoi = 0\n",
    "#     elif nYmax - fXmax > 0:\n",
    "#         start_row_dat = 0\n",
    "#         start_row_aoi = int((nYmax - fYmax) / 30)\n",
    "#     else:\n",
    "#         start_row_dat = 0\n",
    "#         start_row_aoi = 0\n",
    "\n",
    "\n",
    "#     if nYmin - fYmin < 0:\n",
    "#         end_row_dat = values.shape[0]\n",
    "#         end_row_aoi = rows - int((nYmin - fYmin) / 30)\n",
    "#     elif nYmin - fYmin > 0:\n",
    "#         end_row_dat = values.shape[0] - int((nYmin - fYmin) / 30)\n",
    "#         end_row_aoi = rows \n",
    "#     else:\n",
    "#         end_row_dat = values.shape[0]\n",
    "#         end_row_aoi = rows\n",
    "    \n",
    "#     pList = [start_row_dat, end_row_dat, start_col_dat, end_col_dat, start_row_aoi, end_row_aoi, start_col_aoi, end_col_aoi]\n",
    "#     if any(x < 0 for x in pList):\n",
    "#         print('shit')\n",
    "#         continue\n",
    "#     else:\n",
    "#         block[start_row_aoi:end_row_aoi, start_col_aoi:end_col_aoi, i] = values[start_row_dat:end_row_dat, start_col_dat:end_col_dat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106d32fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### take care of overlap due due multiple scenes per day\n",
    "# day_counts = Counter(year_month_path_row)\n",
    "\n",
    "# dummy = 0\n",
    "# cumulative_day_counts_end = []\n",
    "\n",
    "# for _, count in sorted(day_counts.items()):\n",
    "#     dummy += count\n",
    "#     cumulative_day_counts_end.append(dummy)\n",
    "\n",
    "# cumulative_day_counts_start = np.insert(cumulative_day_counts_end, 0 ,0)\n",
    "\n",
    "# # try fancy list aggregation\n",
    "# cumulative_day_counts_start = np.array(cumulative_day_counts_start)\n",
    "# cumulative_day_counts_end = np.array(cumulative_day_counts_end)\n",
    "\n",
    "# ### too slow\n",
    "# stack_list = [\n",
    "#     np.nanmedian(block[:, :, start:end], axis=2)\n",
    "#     for start, end in zip(cumulative_day_counts_start[:-1], cumulative_day_counts_end)\n",
    "# ]\n",
    "\n",
    "# fin_block = np.dstack(stack_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8039e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the function to compute median for one slice\n",
    "# def compute_daily_median(start, end):\n",
    "#     return np.nanmedian(block[:, :, start:end], axis=2)\n",
    "\n",
    "# # Run in parallel\n",
    "# stack_list = Parallel(n_jobs=31)(  # -1 uses all available CPU cores\n",
    "#     delayed(compute_daily_median)(start, end)\n",
    "#     for start, end in zip(cumulative_day_counts_start[:-1], cumulative_day_counts_end)\n",
    "# )\n",
    "\n",
    "# fin_block = np.dstack(stack_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76deec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tiff export\n",
    "# import rasterio\n",
    "# from rasterio.transform import from_origin\n",
    "\n",
    "# # Assuming the dataset has spatial resolution info\n",
    "# transform = from_origin(nXmin, nYmax, res, res)\n",
    "\n",
    "# # Define output GeoTIFF filename\n",
    "# output_filename = f'/data/{origin}et/Landsat/check.tif'\n",
    "\n",
    "# # Open a new GeoTIFF file for writing\n",
    "# with rasterio.open(output_filename, 'w', driver='GTiff', \n",
    "#                    height=fin_block.shape[0], width=fin_block.shape[1],\n",
    "#                    count=1,#fin_block.shape[2], \n",
    "#                    dtype=fin_block.dtype,\n",
    "#                    crs=f'EPSG:{landsat_epsg}', transform=transform) as dst:\n",
    "#     dst.write(fin_block, 1)#fin_block.transpose(2, 0, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
