{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd8f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/potzschf/repos/')\n",
    "from helperToolz.helpsters import *\n",
    "from helperToolz.evapo import *\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "workhorse = True\n",
    "\n",
    "if workhorse:\n",
    "    origin = 'Aldhani/eoagritwin/'\n",
    "else:\n",
    "    origin = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28eb4eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datasets available for training\n",
      "\n",
      "     S2:EVI S2:NDM S2:NDV S2:TCB S2:TCG S2:TCW S3:mean S3:median\n",
      "Year                                                            \n",
      "2017      ✔      ✔      ✔      ✔      ✔      ✔       ✔         ✔\n",
      "2018      ✔      ✔      ✔      ✔      ✔      ✔       ✔         ✔\n",
      "2019      ✔      ✔      ✔      ✔      ✔      ✔       ✔         ✔\n",
      "2020      ✔      ✔      ✔      ✔      ✔      ✔       ✔         ✔\n",
      "2021      ✔      ✔      ✔      ✔      ✔      ✔       ✔         ✔\n",
      "2022      ✔      ✔      ✔      ✔      ✔      ✔       ✔         ✔\n",
      "2023      ✔      ✔      ✔      ✔      ✔      ✔       ✔         ✔\n",
      "2024      ✔      ✔      ✔      ✔      ✔      ✔       ✔         ✔\n"
     ]
    }
   ],
   "source": [
    "# get files and check if years are same for both sensors. Also check what indices and metrics are available\n",
    "training_files = getFilelist('/data/Aldhani/eoagritwin/et/Training_ML/training_data/raw_extracts/', '.parquet')\n",
    "\n",
    "indices_by_year = defaultdict(set)\n",
    "\n",
    "for file in training_files:\n",
    " \n",
    "    match  = re.search(r'/(S2|S3)_(\\d{4})(?:_([A-Za-z]{3,10}))?', file) # (?:_([A-Z]{3}))? --> the ? at the end makes the whole group optional, i.e. also files where \n",
    "                                                                    # only the first 2 groups match will be considered. the first question mark stops the loop from breaking,\n",
    "                                                                    # if a file with only the first 2 groups is encountered\n",
    "    if match:\n",
    "        sensor, year, index = match.groups()\n",
    "        if index:  # Only S2 has metrics in filename\n",
    "            indices_by_year[year].add(f'{sensor}:{index}')\n",
    "    \n",
    "\n",
    "# Get all unique indices\n",
    "all_indices = sorted({index for indices in indices_by_year.values() for index in indices})\n",
    "\n",
    "# Prepare a table: rows = years, columns = indices\n",
    "table_data = []\n",
    "for year in sorted(indices_by_year):\n",
    "    row = {index: '✔' if index in indices_by_year[year] else 'X' for index in all_indices}\n",
    "    row['Year'] = year\n",
    "    table_data.append(row)\n",
    "\n",
    "# Create and show DataFrame\n",
    "df = pd.DataFrame(table_data)\n",
    "df = df.set_index('Year')\n",
    "print('\\nDatasets available for training\\n')\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4438cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring S2 together and in shape\n",
    "index_conti = []\n",
    "for index in [col.split(':')[-1] for col in df.columns[:-2]]:\n",
    "    print(index)\n",
    "    temp_conti = []\n",
    "    for file in training_files:\n",
    "        if index in file:\n",
    "            print(file)\n",
    "            dat = pd.read_parquet(file)\n",
    "            dat = dat.assign(year = re.search(r'_(\\d{4})_', file).group(1))\n",
    "            dat['S2'] = dat['S2'].replace(-9999, np.nan)\n",
    "            dat = dat[['year', 'doy', 'row', 'col', 'S2', 'index']]\n",
    "            dat = dat.rename(columns={'S2': file.split('_')[-1].split('.')[0]})\n",
    "            dat = dat.drop('index', axis=1)\n",
    "            temp_conti.append(dat)\n",
    "    index_conti.append(pd.concat(temp_conti, ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc0ab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "S2_block = index_conti[0]\n",
    "for i in range(1, len(index_conti)):\n",
    "    print(i)\n",
    "    S2_block = pd.merge(S2_block, index_conti[i], on=['row', 'col', 'doy', 'year'], how='inner')\n",
    "S2_block = S2_block.dropna()\n",
    "S2_block.to_parquet(f'/data/{origin}et/Training_ML/training_data/combined_extracts/S2_powerblock.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cf52bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring S3 together and in shape\n",
    "index_conti = []\n",
    "for index in [col.split(':')[-1] for col in df.columns[-2:]]:\n",
    "    print(index)\n",
    "    temp_conti = []\n",
    "    for file in training_files:\n",
    "        if index in file:\n",
    "            print(file)\n",
    "            dat = pd.read_parquet(file)\n",
    "            dat = dat.assign(year = re.search(r'_(\\d{4})_', file).group(1))\n",
    "            dat = dat[['year', 'doy', 'row', 'col', f'S3_{index}']]\n",
    "            dat = dat.rename(columns={f'S3_{index}': f'{index}'})\n",
    "            temp_conti.append(dat)\n",
    "    index_conti.append(pd.concat(temp_conti, ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2353b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_block = pd.merge(index_conti[0], index_conti[1], on=['row', 'col', 'doy', 'year'], how='inner')\n",
    "S3_block = S3_block.dropna()\n",
    "S3_block.to_parquet(f'/data/{origin}et/Training_ML/training_data/combined_extracts/S3_powerblock.parquet', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbd6308",
   "metadata": {},
   "outputs": [],
   "source": [
    "S2_block = pd.read_parquet(f'/data/{origin}et/Training_ML/training_data/combined_extracts/S2_powerblock.parquet')\n",
    "S3_block = pd.read_parquet(f'/data/{origin}et/Training_ML/training_data/combined_extracts/S3_powerblock.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a51ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = pd.merge(S3_block, S2_block, on=['row', 'col', 'doy', 'year'], how='inner')\n",
    "block.to_parquet(f'/data/{origin}et/Training_ML/training_data/combined_extracts/Powerblock.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02a4d9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export little files\n",
    "block['date'] = pd.to_datetime(block['year'], format='%Y') + pd.to_timedelta(block['doy'] - 1, unit='D')\n",
    "block['month'] = block['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0ef26e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n"
     ]
    }
   ],
   "source": [
    "for year in block['year'].unique():\n",
    "    print(year)\n",
    "    for month in block[block['year'] == year]['month'].unique(): # safeguard, if in the first year not all months are present\n",
    "        if not os.path.isfile(f'/data/{origin}et/Training_ML/training_data/combined_extracts_monthly/{year}_{month}.parquet'):\n",
    "            subset = block[(block['year'] == year) & (block['month'] == month)]\n",
    "            subset = subset.drop(['date', 'month'], axis=1)\n",
    "            subset.to_parquet(f'/data/{origin}et/Training_ML/training_data/combined_extracts_monthly/{year}_{month}.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
