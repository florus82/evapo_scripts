{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/potzschf/repos/')\n",
    "from helperToolz.helpsters import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/potzschf/mambaforge/envs/workhorse/lib/python3.12/site-packages/osgeo/ogr.py:601: FutureWarning: Neither ogr.UseExceptions() nor ogr.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "files = sorted(getFilelist('/data/Aldhani/eoagritwin/et/Sentinel3/raw', '.nc'))\n",
    "year = 2018\n",
    "# get a subset of files for that year\n",
    "yearFiles = [file for file in files if int(file.split('/')[-1].split('_')[-1][0:4]) == year]\n",
    "\n",
    "# make dictionary that stores .tif names and related accDates to easier search to close observations with S2 data\n",
    "lookUp = {}\n",
    "\n",
    "int_to_Month = {\n",
    "    '01': 'January',\n",
    "    '02': 'February',\n",
    "    '03': 'March',\n",
    "    '04': 'April',\n",
    "    '05': 'May',\n",
    "    '06': 'June',\n",
    "    '07': 'July',\n",
    "    '08': 'August',\n",
    "    '09': 'September',\n",
    "    '10': 'October',\n",
    "    '11': 'November',\n",
    "    '12': 'December'\n",
    "}\n",
    "\n",
    "# create a maks for germany\n",
    "mask = makeGermanyMaskforNC('/data/Aldhani/eoagritwin/misc/gadm41_DEU_shp/gadm41_DEU_0.shp', yearFiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/Aldhani/eoagritwin/et/Sentinel3/raw/Germany_2018-01.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/raw/Germany_2018-02.nc\n"
     ]
    }
   ],
   "source": [
    "# set storPath for exported tiffs\n",
    "storPath = '/data/Aldhani/eoagritwin/et/Sentinel3/tiffs/'\n",
    "LST_path = f'{storPath}LST/daily_observations_all/{year}/'\n",
    "monthly_composites_path = f'{storPath}LST/monthly_composites/{year}/'\n",
    "os.makedirs(LST_path, exist_ok=True)\n",
    "os.makedirs(monthly_composites_path, exist_ok=True)\n",
    "yearCont = []# for collecting number of observations per year\n",
    "\n",
    "# loop over files and export to .tif at Path locations\n",
    "for i, file in enumerate(yearFiles):\n",
    "    print(file)\n",
    "    if i == 1:\n",
    "        break\n",
    "    accDateTimes = getAllDatesS3(file) # possible to take annual subset if entire files list would be passed here\n",
    "#     convertNCtoTIF(file, LST_path, file.split('/')[-1].split('.')[0] + '.tif', accDateTimes, False, True)\n",
    "\n",
    "    # write filenames and corresponding dates into dictionary for easier datesearch later\n",
    "    lookUp[file.split('.nc')[0]] = accDateTimes\n",
    "\n",
    "    dat = getDataFromNC(file)\n",
    "    monthCont = [] # for collecting number of observations per month\n",
    "    dailyCont = [] # for collecting number of observations per day\n",
    "    dailyVals_median = [] # for collection the actual daily LST values (daily median)\n",
    "    dailyVals_mean = [] # for collection the actual daily LST values (daily mean)\n",
    "    bnames = []\n",
    "    df = pd.Series(accDateTimes)\n",
    "    counts_per_day = df.dt.floor(\"D\").value_counts().sort_index()\n",
    "    # vectors for indexing over days\n",
    "    cumulative_day_counts_end = np.asarray(np.cumsum(counts_per_day))\n",
    "    cumulative_day_counts_start = np.insert(cumulative_day_counts_end, 0 ,0)\n",
    "\n",
    "    # cumulative_day_counts_start = np.array(cumulative_day_counts_start)\n",
    "    # cumulative_day_counts_end = np.array(cumulative_day_counts_end)\n",
    "\n",
    "\n",
    "    ################################################ gives monthly min, max, median composites\n",
    "    # aggreagate by median\n",
    "    # stack_list = [\n",
    "    #     np.nanmedian(dat[:, :, start:end], axis=2)\n",
    "    #     for start, end in zip(cumulative_day_counts_start[:-1], cumulative_day_counts_end)\n",
    "    # ] \n",
    "    # fin_block = np.dstack(stack_list)\n",
    "\n",
    "    MM = int_to_Month[file.rsplit('-', maxsplit=1)[-1].split('.')[0]]\n",
    "    # bands = [f'{MM}_Day_{b+1}' for b in range(fin_block.shape[2])]\n",
    "    # fin_block = fin_block * mask[:, :, np.newaxis]\n",
    "    # fin_block[fin_block == 0] = np.nan\n",
    "    # exportNCarrayDerivatesComp(file, monthly_composites_path, f'Germany_{year}_{MM}_mean.tif', bands, fin_block)\n",
    "\n",
    "    # # aggreagate by min\n",
    "    # stack_list = [\n",
    "    #     np.nanmin(dat[:, :, start:end], axis=2)\n",
    "    #     for start, end in zip(cumulative_day_counts_start[:-1], cumulative_day_counts_end)\n",
    "    # ] \n",
    "    # fin_block = np.dstack(stack_list)\n",
    "    # fin_block = fin_block * mask[:, :, np.newaxis]\n",
    "    # fin_block[fin_block == 0] = np.nan\n",
    "    # exportNCarrayDerivatesComp(file, monthly_composites_path, f'Germany_{year}_{MM}_min.tif', bands, fin_block)\n",
    "\n",
    "    # # aggreagate by max\n",
    "    # stack_list = [\n",
    "    #     np.nanmax(dat[:, :, start:end], axis=2)\n",
    "    #     for start, end in zip(cumulative_day_counts_start[:-1], cumulative_day_counts_end)\n",
    "    # ] \n",
    "    # fin_block = np.dstack(stack_list)\n",
    "    # fin_block = fin_block * mask[:, :, np.newaxis]\n",
    "    # fin_block[fin_block == 0] = np.nan\n",
    "    # exportNCarrayDerivatesComp(file, monthly_composites_path, f'Germany_{year}_{MM}_max.tif', bands, fin_block)\n",
    "\n",
    "    ########################################################### creates metadata raster\n",
    "    for l in range(len(counts_per_day)):\n",
    "        # number of observations per month\n",
    "        monthCont.append(np.any(~np.isnan(dat[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]]),axis=2)) # minimum dail obs\n",
    "        \n",
    "        # collect the dates to use as bandnames for exported tif stacks\n",
    "        bnames.append(str(counts_per_day.index[l].date()))\n",
    "\n",
    "        # collect number of observations per day ( count only one per day!)\n",
    "        dailyCont.append(np.sum(~np.isnan(dat[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]]),axis=2))\n",
    "        \n",
    "        # collect actual LST values\n",
    "        dailyVals_median.append(np.nanmedian(dat[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]], axis = 2))\n",
    "        dailyVals_mean.append(np.nanmean(dat[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]], axis = 2))\n",
    "\n",
    "    # export daily values\n",
    "    exportNCarrayDerivatesInt(file, LST_path, f'Daily_LST_means_{year}_{MM}.tif', bnames, np.dstack(dailyVals_mean), make_uint16=False, numberOfBands=len(dailyVals_mean))\n",
    "    exportNCarrayDerivatesInt(file, LST_path, f'Daily_LST_medians_{year}_{MM}.tif', bnames, np.dstack(dailyVals_median), make_uint16=False, numberOfBands=len(dailyVals_median))\n",
    "\n",
    "    # export day counts\n",
    "    exportNCarrayDerivatesInt(file, storPath + 'Analytics/Count_obs_per_day/', f'Daily_obs_for_{year}_{MM}.tif', bnames, np.dstack(dailyCont), True, numberOfBands=len(dailyCont))\n",
    "    # export month counts\n",
    "    exportNCarrayDerivatesInt(file, storPath + 'Analytics/Count_obs_per_month/', f'Monthly_Min_DailyObs_{('_').join(file.split('_')[-1].split('-')[:2])}.tif', 'monthly_sum_of_daily_obs', np.nansum(np.dstack((monthCont)), axis = 2), True)\n",
    "    \n",
    "    # collect number of observations per year ( count only one per day!)\n",
    "    yearCont.append(np.nansum(np.dstack((monthCont)), axis = 2))\n",
    "\n",
    "# export year counts\n",
    "exportNCarrayDerivatesInt(file, storPath + 'Analytics/Count_obs_per_year/', f'Annual_Min_DailyObs_{file.split('_')[-1].split('-')[0]}.tif', 'annual_sum_of_daily_obs', np.nansum(np.dstack((yearCont)), axis = 2), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3930899/3208845463.py:1: RuntimeWarning: All-NaN slice encountered\n",
      "  np.nanmedian(dat[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]], axis = 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[    nan,     nan,     nan, ...,     nan,     nan,     nan],\n",
       "       [    nan,     nan,     nan, ...,     nan,     nan,     nan],\n",
       "       [    nan,     nan,     nan, ...,     nan,     nan,     nan],\n",
       "       ...,\n",
       "       [    nan,     nan,     nan, ..., 274.42 , 274.86 , 274.8  ],\n",
       "       [    nan,     nan,     nan, ..., 273.332, 273.332, 274.942],\n",
       "       [    nan,     nan,     nan, ..., 273.846, 273.846, 272.472]],\n",
       "      shape=(893, 1083), dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
