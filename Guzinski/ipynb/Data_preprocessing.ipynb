{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/potzschf/repos/')\n",
    "from helperToolz.helpsters import *\n",
    "from helperToolz.evapo import *\n",
    "\n",
    "workhorse = True\n",
    "\n",
    "if workhorse:\n",
    "    origin = 'Aldhani/eoagritwin/'\n",
    "else:\n",
    "    origin = ''\n",
    "\n",
    "import re\n",
    "from other_repos.pyDMS.pyDMS.pyDMS import *\n",
    "\n",
    "int_to_Month = {\n",
    "    '01': 'January',\n",
    "    '02': 'February',\n",
    "    '03': 'March',\n",
    "    '04': 'April',\n",
    "    '05': 'May',\n",
    "    '06': 'June',\n",
    "    '07': 'July',\n",
    "    '08': 'August',\n",
    "    '09': 'September',\n",
    "    '10': 'October',\n",
    "    '11': 'November',\n",
    "    '12': 'December'\n",
    "    }\n",
    "\n",
    "dayCount_leap = [31,29,31,30,31,30,31,31,30,31,30,31]\n",
    "dayCount_noleap = [31,28,31,30,31,30,31,31,30,31,30,31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb77c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = f'/data/{origin}force/output/Guzinski/'\n",
    "# define the year you are working on \n",
    "year = 2019\n",
    "\n",
    "# first, all files for months outside of studay aim will be deleted to save storgage\n",
    "reduce_forceTSI_output_to_validmonths(f'{base_path}{year}/tiles', 4, 10)\n",
    "print('All FORCE files outside April-October deleted')\n",
    "# get a list with all available tiles\n",
    "files = getFilelist(f'{base_path}{year}/tiles', '.tif', deep=True) \n",
    "unique_tiles = get_forceTSI_output_Tiles(files)\n",
    "print(f'There are {len(unique_tiles)} tiles available for processing for the year {year}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe08da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the tiles to process and check if they contain composites for the same dates so that they can be stacked in the mosaiked\n",
    "# (there is no sanity check for mosaiking)\n",
    "\n",
    "tiles_to_process = unique_tiles#createFORCEtileLIST([67], [41])\n",
    "date_list = []\n",
    "for tile in tiles_to_process:\n",
    "    date_list.append((get_forceTSI_output_DOYS([file for file in files if tile in file])))\n",
    "for i in range(0,len(date_list)-1):\n",
    "    if date_list[i] == date_list[i + 1]:\n",
    "        continue\n",
    "    else:\n",
    "        print('the doys of composites across tiles is not equal - Better check!!!!!')\n",
    "date_list = date_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886991ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the LST file, aspect, ratio and incedence for the tile/time and stackem\n",
    "# loop over the adequate bands from monthly LST stack (each band corresponds to the respective day of that month), asepct, ratio & incidence files\n",
    "\n",
    "comp_stat = 'max'\n",
    "LST_path = '/data/Aldhani/eoagritwin/et/Sentinel3/tiffs/LST/daily_observations_all/'\n",
    "\n",
    "date = date_list[0] # will be replaced through loop\n",
    "\n",
    "year = int(date[0:4])\n",
    "\n",
    "# what bands (days) in LST from which month are associated with the current S2 scene\n",
    "if is_leap_year(year):\n",
    "    dayCountList = dayCount_leap\n",
    "    end_day = 367\n",
    "else:\n",
    "    dayCountList = dayCount_noleap\n",
    "    end_day = 366\n",
    "\n",
    "day_sequence = [f'{year}{month:02d}{day:02d}' \n",
    "        for month, days_in_month in enumerate(dayCountList, start=1)\n",
    "        for day in range(1, days_in_month + 1)]\n",
    "    \n",
    "dicti = {}\n",
    "\n",
    "for i in range(1, end_day):\n",
    "    month_idx = np.where(np.cumsum(dayCountList) >= i)[0][0]\n",
    "    day_in_month =  i - (np.cumsum(dayCountList)[month_idx - 1] if month_idx > 0 else 0)\n",
    "\n",
    "    dicti[i] = {\n",
    "        'month': int_to_Month[f'{(month_idx + 1):02d}'],\n",
    "        'band': day_in_month\n",
    "    }\n",
    "\n",
    "index_in_sequence = [i+1 for i, day in enumerate(day_sequence) if day == date][0]\n",
    "print(index_in_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ed578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only working for growing season, does not work across years!!!!!!\n",
    "for idx in range(index_in_sequence - 4, index_in_sequence + 5):\n",
    "    if dicti[idx]['month'] in ['February', 'November']:\n",
    "        continue\n",
    "    else:\n",
    "        print(dicti[idx]['month'])\n",
    "        print(dicti[idx]['band'])\n",
    "    #     ds = gdal.Open(f'/data/Aldhani/eoagritwin/et/Sentinel3/tiffs/LST/daily_observations_all/{year}/Daily_LST_means_year_{dicti[idx]['month']}.tif', 0)\n",
    "    #     LST_arr = ds.GetRasterBand(dicti[idx]['band']) # store as single Tiff in temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f809b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dates from FORCE 10 day interpolations and make stacked tifs\n",
    "for tile in tiles:\n",
    "  for unique_day in unique_days:\n",
    "    stack_tifs(sorted([file for file in files if unique_day in file and tile in file]), f'/data/{origin}force/output/Guzinski/1TileTest/{tile}_{unique_day}.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd543292",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_2Tiles = sorted([file for file in files for tile in tiles if tile in file and unique_days[12] in file])\n",
    "files_2Tiles = [files_2Tiles[i] for i in [0,10,1,11,2,12,3,13,4,14,5,15,6,16,7,17,8,18]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a44dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_2Tiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd46b5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ed9b52",
   "metadata": {},
   "outputs": [],
   "source": [
    " # tiles = list(set([file.split('output/')[-1].split('/')[1].split('/')[0] for file in list_of_forcefiles]))\n",
    "    \n",
    "\n",
    "        # make paths in vrts relative\n",
    "        vrts = getFilelist(outDir, '.vrt')\n",
    "        for vrt in vrts:\n",
    "            convertVRTpathsTOrelative(vrt)\n",
    "        nums = [int(vrt.split('_')[-1].split('.')[0]) for vrt in vrts]\n",
    "        vrts_sorted = sortListwithOtherlist(nums, vrts)[-1]\n",
    "        print('paths in vrts made relative')\n",
    "        \n",
    "        vrt = gdal.BuildVRT(f'{outDir}{force_folder_name}_Cube.vrt', vrts_sorted, separate = True)\n",
    "        vrt = None\n",
    "        # convertVRTpathsTOrelative(f'{outDir}{force_folder_name}_Cube.vrt')\n",
    "        print('overlord vrt created')\n",
    "        if pyramids:\n",
    "            # build pyramids\n",
    "            vrtPyramids(f'{outDir}{force_folder_name}_Cube.vrt')\n",
    "            print('VRT created with pyramids')\n",
    "    else:\n",
    "        print('Vrt might already exist - please check!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe22669",
   "metadata": {},
   "outputs": [],
   "source": [
    "highResFilename1 = '/data/Aldhani/eoagritwin/force/output/Guzinski/1TileTest/X0067_Y0042_20190630.tif'\n",
    "highResFilename2 = '/data/Aldhani/eoagritwin/force/output/Guzinski/1TileTest/X0068_Y0042_20190630.tif'\n",
    "lowResFilename = '/data/Aldhani/eoagritwin/et/Sentinel3/tiffs/LST/daily_observations_all/2019/Daily_LST_means_2019_July.tif'\n",
    "\n",
    "outputFilename = '/data/Aldhani/eoagritwin/et/Sentinel3/tiffs/LST/test6_1.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e9f99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "useDecisionTree = True\n",
    "\n",
    "commonOpts = {\"highResFiles\":               [highResFilename1, highResFilename2],#[highResFilename],\n",
    "                \"lowResFiles\":              [lowResFilename, lowResFilename],\n",
    "                \"lowResQualityFiles\":         [],# [lowResMaskFilename],\n",
    "                \"lowResGoodQualityFlags\":     [],#[255],\n",
    "                \"cvHomogeneityThreshold\":     0,\n",
    "                \"movingWindowSize\":           15,\n",
    "                \"disaggregatingTemperature\":  True}\n",
    "dtOpts =     {\"perLeafLinearRegression\":    True,\n",
    "                \"linearRegressionExtrapolationRatio\": 0.25}\n",
    "sknnOpts =   {'hidden_layer_sizes':         (10,),\n",
    "                'activation':                 'tanh'}\n",
    "nnOpts =     {\"regressionType\":             REG_sklearn_ann,\n",
    "                \"regressorOpt\":               sknnOpts}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if useDecisionTree:\n",
    "    opts = commonOpts.copy()\n",
    "    opts.update(dtOpts)\n",
    "    disaggregator = DecisionTreeSharpener(**opts)\n",
    "else:\n",
    "    opts = commonOpts.copy()\n",
    "    opts.update(nnOpts)\n",
    "    disaggregator = NeuralNetworkSharpener(**opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976021f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training regressor...\")\n",
    "disaggregator.trainSharpener()\n",
    "print(\"Sharpening...\")\n",
    "downscaledFile = disaggregator.applySharpener(highResFilename1, lowResFilename)\n",
    "print(\"Residual analysis...\")\n",
    "residualImage, correctedImage = disaggregator.residualAnalysis(downscaledFile, lowResFilename,\n",
    "                                                               # lowResMaskFilename,\n",
    "                                                                doCorrection=True)\n",
    "print(\"Saving output...\")\n",
    "highResFile = gdal.Open(highResFilename1)\n",
    "if correctedImage is not None:\n",
    "    outImage = correctedImage\n",
    "else:\n",
    "    outImage = downscaledFile\n",
    "# outData = utils.binomialSmoother(outData)\n",
    "outFile = utils.saveImg(outImage.GetRasterBand(1).ReadAsArray(),\n",
    "                        outImage.GetGeoTransform(),\n",
    "                        outImage.GetProjection(),\n",
    "                        outputFilename)\n",
    "residualFile = utils.saveImg(residualImage.GetRasterBand(1).ReadAsArray(),\n",
    "                            residualImage.GetGeoTransform(),\n",
    "                            residualImage.GetProjection(),\n",
    "                            os.path.splitext(outputFilename)[0] + \"_residual\" +\n",
    "                            os.path.splitext(outputFilename)[1])\n",
    "\n",
    "outFile = None\n",
    "residualFile = None\n",
    "downsaceldFile = None\n",
    "highResFile = None\n",
    "\n",
    "print(time.time() - start_time, \"seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
