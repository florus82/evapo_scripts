{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da21555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/potzschf/repos/')\n",
    "from helperToolz.helpsters import *\n",
    "from helperToolz.dicts_and_lists import *\n",
    "from helperToolz.guzinski import * \n",
    "from other_repos.pyDMS.pyDMS.pyDMS import *\n",
    "import time\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "os.environ[\"GDAL_MAX_DATASET_POOL_SIZE\"] = \"600\"\n",
    "\n",
    "\n",
    "def runSharpi(highResFilename, lowResFilename, lowResMaskFilename, cv, movWin, regrat, outputFilename, useDecisionTree = True):\n",
    "    commonOpts = {\"highResFiles\":               [highResFilename],\n",
    "                    \"lowResFiles\":              [lowResFilename],\n",
    "                    \"lowResQualityFiles\":         [lowResMaskFilename],\n",
    "                    \"lowResGoodQualityFlags\":     [1],\n",
    "                    \"cvHomogeneityThreshold\":     cv,\n",
    "                    \"movingWindowSize\":           movWin,\n",
    "                    \"disaggregatingTemperature\":  True}\n",
    "    dtOpts =     {\"perLeafLinearRegression\":    True,\n",
    "                    \"linearRegressionExtrapolationRatio\": round(regrat, 2)}\n",
    "    sknnOpts =   {'hidden_layer_sizes':         (10,),\n",
    "                    'activation':                 'tanh'}\n",
    "    nnOpts =     {\"regressionType\":             REG_sklearn_ann,\n",
    "                    \"regressorOpt\":               sknnOpts}\n",
    "\n",
    "    start_time = time.time()\n",
    "    opts = commonOpts.copy()\n",
    "    if useDecisionTree:\n",
    "        opts.update(dtOpts)\n",
    "        disaggregator = DecisionTreeSharpener(**opts)\n",
    "    else:\n",
    "        opts.update(nnOpts)\n",
    "        disaggregator = NeuralNetworkSharpener(**opts)\n",
    "\n",
    "    print(\"Training regressor...\")\n",
    "    disaggregator.trainSharpener()\n",
    "    print(\"Sharpening...\")\n",
    "    downscaledFile = disaggregator.applySharpener(highResFilename, lowResFilename)\n",
    "    print(\"Residual analysis...\")\n",
    "    residualImage, correctedImage = disaggregator.residualAnalysis(downscaledFile, lowResFilename,\n",
    "                                                                    lowResMaskFilename,\n",
    "                                                                    doCorrection=True)\n",
    "    print(\"Saving output...\")\n",
    "\n",
    "    if correctedImage is not None:\n",
    "        outImage = correctedImage\n",
    "    else:\n",
    "        outImage = downscaledFile\n",
    "    # outData = utils.binomialSmoother(outData)\n",
    "    outFile = utils.saveImg(outImage.GetRasterBand(1).ReadAsArray(),\n",
    "                            outImage.GetGeoTransform(),\n",
    "                            outImage.GetProjection(),\n",
    "                            f'{os.path.split(outputFilename)[0]}/Values/{os.path.split(outputFilename)[1]}')\n",
    "    residualFile = utils.saveImg(residualImage.GetRasterBand(1).ReadAsArray(),\n",
    "                                residualImage.GetGeoTransform(),\n",
    "                                residualImage.GetProjection(),\n",
    "                                f'{os.path.split(outputFilename)[0]}/Residuals/{os.path.split(outputFilename)[1]}_resid{os.path.splitext(outputFilename)[1]}')\n",
    "\n",
    "    outFile = None\n",
    "    residualFile = None\n",
    "    downsaceldFile = None\n",
    "   \n",
    "    print(time.time() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c702a958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60]\n",
      "[40]\n",
      "[61]\n",
      "[40]\n",
      "[62]\n",
      "[40]\n",
      "[63]\n",
      "[40]\n",
      "[64]\n",
      "[40]\n",
      "[65]\n",
      "[40]\n",
      "[66]\n",
      "[40]\n",
      "[67]\n",
      "[40]\n",
      "[68]\n",
      "[40]\n",
      "[69]\n",
      "[40]\n",
      "[70]\n",
      "[40]\n",
      "[60]\n",
      "[41]\n",
      "[61]\n",
      "[41]\n",
      "[62]\n",
      "[41]\n",
      "[63]\n",
      "[41]\n",
      "[64]\n",
      "[41]\n",
      "[65]\n",
      "[41]\n",
      "[66]\n",
      "[41]\n",
      "[67]\n",
      "[41]\n",
      "[68]\n",
      "[41]\n",
      "[69]\n",
      "[41]\n",
      "[70]\n",
      "[41]\n",
      "[60]\n",
      "[42]\n",
      "[61]\n",
      "[42]\n",
      "[62]\n",
      "[42]\n",
      "[63]\n",
      "[42]\n",
      "[64]\n",
      "[42]\n",
      "[65]\n",
      "[42]\n",
      "[66]\n",
      "[42]\n",
      "[67]\n",
      "[42]\n",
      "[68]\n",
      "[42]\n",
      "[69]\n",
      "[42]\n",
      "[70]\n",
      "[42]\n",
      "[60]\n",
      "[43]\n",
      "[61]\n",
      "[43]\n",
      "[62]\n",
      "[43]\n",
      "[63]\n",
      "[43]\n",
      "[64]\n",
      "[43]\n",
      "[65]\n",
      "[43]\n",
      "[66]\n",
      "[43]\n",
      "[67]\n",
      "[43]\n",
      "[68]\n",
      "[43]\n",
      "[69]\n",
      "[43]\n",
      "[70]\n",
      "[43]\n"
     ]
    }
   ],
   "source": [
    "# paths\n",
    "lowmask_path = '/data/Aldhani/eoagritwin/et/Auxiliary/DEM/reprojected/THUENEN_GER_LST_WARP.tif'\n",
    "lowmask_bin_path = '/data/Aldhani/eoagritwin/et/Auxiliary/DEM/reprojected/THUENEN_GER_LST_WARP_BINARY.tif'\n",
    "temp_dump = '/data/Aldhani/eoagritwin/et/Sentinel3//LST/LST_values/tempDump/'\n",
    "\n",
    "# create vrts of slope, aspect and landcover (for masking)\n",
    "path_to_slope = '/data/Aldhani/eoagritwin/et/Auxiliary/DEM/Force_Tiles/SLOPE/'\n",
    "path_to_aspect = '/data/Aldhani/eoagritwin/et/Auxiliary/DEM/Force_Tiles/ASPECT/'\n",
    "path_to_agro = '/data/Aldhani/eoagritwin/et/Auxiliary/DEM/Force_Tiles/THUENEN_2021/'\n",
    "\n",
    "# get all highRes datasets: S2 composites, aspect, ratio and incedence for the tile/time and stack them (in a composite if more than one tile is processed)\n",
    "# set the highRes S2 tiles that will be processed\n",
    "# tiles_to_process = createFORCEtileLIST(list(bran['Tile_X']),\n",
    "#                                         list(bran['Tile_Y']))\n",
    "\n",
    "\n",
    "\n",
    "Tiles_X = [[i] for i in range(60,71,1)] * 4\n",
    "Tiles_Y = [[i] for i in range(40,44,1) for _ in range(int(len(Tiles_X)/4))]\n",
    "\n",
    "for Tile_X, Tile_Y in zip(Tiles_X, Tiles_Y):\n",
    "    print(Tile_X)\n",
    "    print(Tile_Y)\n",
    "\n",
    "    tiles_to_process = createFORCEtileLIST(Tile_X, Tile_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a4c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a specific folder for this run and store the info together\n",
    "csv_path = f'{temp_dump}folders.csv'\n",
    "rand_foldname = getUniqueIDfromTILESXY(Tile_X, Tile_Y)\n",
    "temp_dump_fold = f'{temp_dump}{rand_foldname}/'\n",
    "\n",
    "if not os.path.exists(temp_dump_fold):\n",
    "    os.makedirs(temp_dump_fold,exist_ok=False)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "            'Folder': temp_dump_fold,\n",
    "            'Tile_X': Tile_X,\n",
    "            'Tile_Y': Tile_Y   \n",
    "        })\n",
    "\n",
    "    if not os.path.exists(csv_path):\n",
    "        df.to_csv(csv_path, index=False)\n",
    "    else:\n",
    "        df_exist = pd.read_csv(csv_path)\n",
    "        df_new = pd.concat([df_exist, df], ignore_index=True)\n",
    "        df_new.to_csv(csv_path, index=False)\n",
    "        # add the lines\n",
    "\n",
    "    # slope-tiles\n",
    "    slopes = [file for file in getFilelist(path_to_slope, '.tif') if any(tile in file for tile in tiles_to_process)] # if any tile name is in file\n",
    "    # aspect-tiles\n",
    "    aspects = [file for file in getFilelist(path_to_aspect, '.tif') if any(tile in file for tile in tiles_to_process)] # if any tile name is in file\n",
    "    # thuenen-tiles\n",
    "    thuenen = [file for file in getFilelist(path_to_agro, '.tif') if any(tile in file for tile in tiles_to_process)] # if any tile name is in file\n",
    "\n",
    "    # get those tiles (and composite if more than one tile is provided)\n",
    "    slope_path = f'{temp_dump_fold}SLOPE.vrt'\n",
    "    gdal.BuildVRT(slope_path, slopes)\n",
    "\n",
    "    aspect_path = f'{temp_dump_fold}ASPECT.vrt'\n",
    "    gdal.BuildVRT(aspect_path, aspects)\n",
    "\n",
    "    thuenen_path = f'{temp_dump_fold}THUENEN.vrt'\n",
    "    gdal.BuildVRT(thuenen_path, thuenen)\n",
    "else:\n",
    "    print('Tile combination already processed before - slope and aspect vrt should already be present')\n",
    "    slope_path = f'{temp_dump_fold}SLOPE_vrt'\n",
    "    aspect_path = f'{temp_dump_fold}ASPECT.vrt'\n",
    "    thuenen_path = f'{temp_dump_fold}THUENEN.vrt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b749d1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all dates of composites are the same :)\n"
     ]
    }
   ],
   "source": [
    "# year \n",
    "year = 2019\n",
    "  \n",
    "# paths\n",
    "path_to_S2_tiles = f'/data/Aldhani/eoagritwin/force/output/Guzinski/{year}/'\n",
    "\n",
    "##### which tiles should be processed\n",
    "# get a list with all available tiles\n",
    "files = getFilelist(f'{path_to_S2_tiles}/tiles', '.tif', deep=True) \n",
    "files = [file for file in files if any(tile in file for tile in tiles_to_process)]\n",
    "date_list = check_forceTSI_compositionDates(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5370bd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20181201',\n",
       " '20181210',\n",
       " '20181219',\n",
       " '20181228',\n",
       " '20190106',\n",
       " '20190115',\n",
       " '20190124',\n",
       " '20190202',\n",
       " '20190211',\n",
       " '20190220',\n",
       " '20190301',\n",
       " '20190310',\n",
       " '20190319',\n",
       " '20190328',\n",
       " '20190406',\n",
       " '20190415',\n",
       " '20190424',\n",
       " '20190503',\n",
       " '20190512',\n",
       " '20190521',\n",
       " '20190530',\n",
       " '20190608',\n",
       " '20190617',\n",
       " '20190626',\n",
       " '20190705',\n",
       " '20190714',\n",
       " '20190723',\n",
       " '20190801',\n",
       " '20190810',\n",
       " '20190819',\n",
       " '20190828',\n",
       " '20190906',\n",
       " '20190915',\n",
       " '20190924',\n",
       " '20191003',\n",
       " '20191012',\n",
       " '20191021',\n",
       " '20191030',\n",
       " '20191108',\n",
       " '20191117',\n",
       " '20191126',\n",
       " '20191205',\n",
       " '20191214',\n",
       " '20191223',\n",
       " '20200101',\n",
       " '20200110',\n",
       " '20200119']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a981e2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all dates of composites are the same :)\n"
     ]
    }
   ],
   "source": [
    "# year \n",
    "year = 2019\n",
    "  \n",
    "# paths\n",
    "path_to_S2_tiles = f'/data/Aldhani/eoagritwin/force/output/Guzinski/{year}/'\n",
    "\n",
    "##### which tiles should be processed\n",
    "# get a list with all available tiles\n",
    "files = getFilelist(f'{path_to_S2_tiles}/tiles', '.tif', deep=True) \n",
    "files = [file for file in files if any(tile in file for tile in tiles_to_process)]\n",
    "date_list = check_forceTSI_compositionDates(files)\n",
    "\n",
    "th_ds = gdal.Open(thuenen_path)\n",
    "th_arr = th_ds.GetRasterBand(1).ReadAsArray()\n",
    "mask = np.where(th_arr == -9999, 0, 1)\n",
    "\n",
    "lowRes_files = []\n",
    "highRes_files = []\n",
    "highRes_names = []\n",
    "\n",
    "colors = ['BLU', 'GRN', 'RED', 'NIR', 'RE1', 'RE2', 'RE3',  'SW1', 'SW2']\n",
    "\n",
    "#### S2 composites are time sensitive (need to be aligned with date of LST observation), so is incidence\n",
    "for date in date_list:\n",
    "\n",
    "    # if not os.path.exists(f'{temp_dump_fold}INCIDENCE_{date}.vrt'):\n",
    "    if date != '20190512':\n",
    "        continue\n",
    "\n",
    "    # check if for that date vrts were already processed\n",
    "    if os.path.exists(f'{temp_dump_fold}S2_{date}'):\n",
    "        print('S2 data for this date already processed')\n",
    "        not_masked = [file for file in getFilelist(temp_dump_fold, '.vrt') if 'HIGHRES' in file]\n",
    "        masked = [file for file in getFilelist(temp_dump_fold, '.tif') if 'HIGHRES' in file]\n",
    "        highRes_files = [item for pair in zip(not_masked, masked) for item in pair]\n",
    "        highRes_names = [\"S2notMasked\" if \".vrt\" in file else \"S2Masked\" for file in highRes_files]\n",
    "        lowRes_files = [f for f in [file for file in getFilelist(temp_dump_fold, '.tif') if 'Daily_LST' in file] for _ in range(2)]\n",
    "        continue\n",
    "    else:\n",
    "        # get those tiles (and composite if more than one tile is provided)\n",
    "        if len(tiles_to_process) == 1:\n",
    "\n",
    "            tilesS2 = [file for file in getFilelist(path_to_S2_tiles, '.tif', deep=True) if tiles_to_process[0] in file and f'{date}.tif' in file]\n",
    "            tilesS2 = [t2 for col in colors for t2 in tilesS2 if col in t2]\n",
    "            S2_path = f'{temp_dump_fold}S2_{date}.vrt'\n",
    "            vrt = gdal.BuildVRT(S2_path, tilesS2, separate=True)\n",
    "            vrt = None\n",
    "            vrt = gdal.Open(S2_path, gdal.GA_Update)  # VRT must be writable\n",
    "            for idz, bname in enumerate(colors): \n",
    "                band = vrt.GetRasterBand(1+idz)\n",
    "                band.SetDescription(bname)\n",
    "            vrt = None\n",
    "\n",
    "        else:\n",
    "            tilesS2 = [file for file in getFilelist(path_to_S2_tiles, '.tif', deep=True) if any(tile in file for tile in tiles_to_process) and f'{date}.tif' in file] \n",
    "            force_to_vrt(tilesS2,\n",
    "                    getCOLORSinOrderFORCELIST(tilesS2, colors, single=False),\n",
    "                    f'{temp_dump_fold}S2_{date}',\n",
    "                    False,\n",
    "                    bandnames= list(dict.fromkeys(tile.split('SEN2L_')[-1].split('_TSI')[0] for tile in tilesS2)))\n",
    "            \n",
    "            S2_path = [file for file in getFilelist(f'{temp_dump_fold}S2_{date}', '.vrt', deep=True) if '_Cube' in file][0]\n",
    "            \n",
    "            # determine LST and incidence files associated with respective S2 composite\n",
    "        band_dict = transform_compositeDate_into_LSTbands(date, 4)\n",
    "\n",
    "\n",
    "        # stat used for compositing\n",
    "        for comp_stat in ['minVZA', 'maxLST']:\n",
    "            path_to_incident = f'/data/Aldhani/eoagritwin/et/Auxiliary/DEM/Force_Tiles/INCIDENCE/{comp_stat}/{year}/'\n",
    "            path_to_LST = f'/data/Aldhani/eoagritwin/et/Sentinel3/LST/LST_values/LST_composites/{comp_stat}/{year}/'\n",
    "\n",
    "            # get all LST bands that can be sharped with the S2 composite at this date (and sun angle incidence files as well, as they are dependent on that date\n",
    "            LSTs = []\n",
    "\n",
    "            for k, v in band_dict.items():\n",
    "                month = v['month']\n",
    "                band = int(v['band'])\n",
    "                v_path = f'{path_to_LST}Daily_LST_{comp_stat}_{year}_{month}.tif'\n",
    "                ds = gdal.Open(v_path, 0)\n",
    "                \n",
    "                # export the LST for that day\n",
    "                LST_arr = ds.GetRasterBand(band).ReadAsArray() # store as single Tiff in temp\n",
    "                makeTif_np_to_matching_tif(LST_arr, v_path, f'{temp_dump_fold}Daily_LST_{comp_stat}_{year}_{month}_{band:02d}.tif')\n",
    "\n",
    "                # store the paths for selecting incidence for corresponding LST\n",
    "                incid_date = f'{year}_{month}_{band:02d}.tif'\n",
    "                lowRes_files.append(f'{temp_dump_fold}Daily_LST_{comp_stat}_{year}_{month}_{band:02d}.tif')\n",
    "\n",
    "                # incidence-tiles\n",
    "                incids = [file for file in getFilelist(path_to_incident, '.tif', deep=True) if any(tile in file for tile in tiles_to_process) and incid_date in file] \n",
    "                # get those tiles (and composite if more than one tile is provided)\n",
    "                if len(tiles_to_process) == 1:\n",
    "                    incid_path = incids[0]\n",
    "\n",
    "                else:\n",
    "                    incid_path = f'{temp_dump_fold}INCIDENCE_{comp_stat}_{incid_date.split('.')[0]}.vrt'\n",
    "                    gdal.BuildVRT(incid_path, incids)\n",
    "\n",
    "                # create highRes file through exapnding the vrt of S2\n",
    "                highRes_path = f'{temp_dump_fold}HIGHRES_{comp_stat}_{incid_date.split('.')[0]}.vrt'\n",
    "                gdal.BuildVRT(highRes_path, [S2_path, slope_path, aspect_path, incid_path], separate=True)\n",
    "                maskVRT_water(highRes_path)\n",
    "                highRes_files.append(f'{highRes_path.split('.')[0]}_watermask.tif')\n",
    "                highRes_names.append('S2notMasked')\n",
    "                maskVRT(f'{highRes_path.split('.')[0]}_watermask.tif', mask)\n",
    "                highRes_files.append(f'{highRes_path.split('.')[0]}_watermask_S2_agromask.tif')\n",
    "                lowRes_files.append(f'{temp_dump_fold}Daily_LST_{comp_stat}_{year}_{month}_{band:02d}.tif')\n",
    "                highRes_names.append('S2Masked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d35e81a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20181201',\n",
       " '20181210',\n",
       " '20181219',\n",
       " '20181228',\n",
       " '20190106',\n",
       " '20190115',\n",
       " '20190124',\n",
       " '20190202',\n",
       " '20190211',\n",
       " '20190220',\n",
       " '20190301',\n",
       " '20190310',\n",
       " '20190319',\n",
       " '20190328',\n",
       " '20190406',\n",
       " '20190415',\n",
       " '20190424',\n",
       " '20190503',\n",
       " '20190512',\n",
       " '20190521',\n",
       " '20190530',\n",
       " '20190608',\n",
       " '20190617',\n",
       " '20190626',\n",
       " '20190705',\n",
       " '20190714',\n",
       " '20190723',\n",
       " '20190801',\n",
       " '20190810',\n",
       " '20190819',\n",
       " '20190828',\n",
       " '20190906',\n",
       " '20190915',\n",
       " '20190924',\n",
       " '20191003',\n",
       " '20191012',\n",
       " '20191021',\n",
       " '20191030',\n",
       " '20191108',\n",
       " '20191117',\n",
       " '20191126',\n",
       " '20191205',\n",
       " '20191214',\n",
       " '20191223',\n",
       " '20200101',\n",
       " '20200110',\n",
       " '20200119']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed04290",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblist = []\n",
    "outFolder = f'/data/Aldhani/eoagritwin/et/Sentinel3/LST/LST_values/sharpened/{rand_foldname}/'\n",
    "for idx, highResFilename in enumerate(highRes_files):\n",
    "    lowResFilename = lowRes_files[idx]\n",
    "    f1 = f'{outFolder}{'/'.join(highResFilename.split('.')[0].split('_')[2:6])}/'\n",
    "    for maskname, mask_lowRes in zip(['withoutLSTmask', 'withLSTmask'], ['', lowmask_bin_path]):\n",
    "        lowResMaskFilename = mask_lowRes\n",
    "        f2 = f'{f1}{maskname}/'\n",
    "        for movWin in [15]:\n",
    "            for cv in [0]:\n",
    "                for regrat in [0.25]:\n",
    "                    kombi = f'mvwin{movWin}_cv{cv}_regrat{int(regrat*100):02d}_{highRes_names[idx]}_{maskname}'\n",
    "                    f3 = f'{f2}{highRes_names[idx]}/'\n",
    "                    os.makedirs(f'{f3}Residuals/', exist_ok=True)\n",
    "                    os.makedirs(f'{f3}Values/', exist_ok=True)\n",
    "                    joblist.append([highResFilename, \n",
    "                                lowResFilename,\n",
    "                                lowResMaskFilename,\n",
    "                                cv, movWin, regrat,\n",
    "                                f'{f3}{'_'.join(highResFilename.split('.')[0].split('_')[2:6])}_{kombi}.tif'])\n",
    "\n",
    "print(f'\\n{len(joblist)} times will be sharpened\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5735a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    starttime = time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Starting process, time:\" + starttime)\n",
    "    print(\"\")\n",
    "   \n",
    "    Parallel(n_jobs=20)(delayed(runSharpi)(job[0], job[1], job[2], job[3], job[4], job[5], job[6]) for job in joblist)\n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "    endtime = time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"start : \" + starttime)\n",
    "    print(\"end: \" + endtime)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48faf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script sharper_test.ipynb --output /home/potzschf/repos/evapo_scripts/Guzinski/py/sharper_test_in_PARALLEL_14_09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4972e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.ones((3,4,23))\n",
    "print(aa.shape)\n",
    "\n",
    "bb = aa[:,:,0:9]\n",
    "print(bb.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
