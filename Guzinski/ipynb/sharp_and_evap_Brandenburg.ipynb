{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6fcd5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/potzschf/repos/')\n",
    "from helperToolz.helpsters import *\n",
    "from helperToolz.dicts_and_lists import *\n",
    "from helperToolz.guzinski import * \n",
    "from helperToolz.mirmazloumi import *\n",
    "from other_repos.pyDMS.pyDMS.pyDMS import *\n",
    "from other_repos.pyTSEB.pyTSEB import meteo_utils\n",
    "from other_repos.pyTSEB.pyTSEB import resistances\n",
    "from other_repos.pyTSEB.pyTSEB import net_radiation\n",
    "from other_repos.pyTSEB.pyTSEB import clumping_index \n",
    "from other_repos.pyTSEB.pyTSEB import TSEB\n",
    "import time\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ca9bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runSharpi(highResFilename, lowResFilename, lowResMaskFilename, cv, movWin, regrat, outputFilename, useDecisionTree = True):\n",
    "    commonOpts = {\"highResFiles\":               [highResFilename],\n",
    "                    \"lowResFiles\":              [lowResFilename],\n",
    "                    \"lowResQualityFiles\":         [lowResMaskFilename],\n",
    "                    \"lowResGoodQualityFlags\":     [1],\n",
    "                    \"cvHomogeneityThreshold\":     cv,\n",
    "                    \"movingWindowSize\":           movWin,\n",
    "                    \"disaggregatingTemperature\":  True}\n",
    "    dtOpts =     {\"perLeafLinearRegression\":    True,\n",
    "                    \"linearRegressionExtrapolationRatio\": round(regrat, 2)}\n",
    "    sknnOpts =   {'hidden_layer_sizes':         (10,),\n",
    "                    'activation':                 'tanh'}\n",
    "    nnOpts =     {\"regressionType\":             REG_sklearn_ann,\n",
    "                    \"regressorOpt\":               sknnOpts}\n",
    "\n",
    "    start_time = time.time()\n",
    "    opts = commonOpts.copy()\n",
    "    if useDecisionTree:\n",
    "        opts.update(dtOpts)\n",
    "        disaggregator = DecisionTreeSharpener(**opts)\n",
    "    else:\n",
    "        opts.update(nnOpts)\n",
    "        disaggregator = NeuralNetworkSharpener(**opts)\n",
    "\n",
    "\n",
    "    disaggregator.trainSharpener()\n",
    "\n",
    "    downscaledFile = disaggregator.applySharpener(highResFilename, lowResFilename)\n",
    "\n",
    "    residualImage, correctedImage = disaggregator.residualAnalysis(downscaledFile, lowResFilename,\n",
    "                                                                    lowResMaskFilename,\n",
    "                                                                    doCorrection=True)\n",
    "\n",
    "\n",
    "    if correctedImage is not None:\n",
    "        outImage = correctedImage\n",
    "    else:\n",
    "        outImage = downscaledFile\n",
    "    # outData = utils.binomialSmoother(outData)\n",
    "    outFile = utils.saveImg(outImage.GetRasterBand(1).ReadAsArray(),\n",
    "                            outImage.GetGeoTransform(),\n",
    "                            outImage.GetProjection(),\n",
    "                            f'{os.path.split(outputFilename)[0]}/Values/{os.path.split(outputFilename)[1]}')\n",
    "    residualFile = utils.saveImg(residualImage.GetRasterBand(1).ReadAsArray(),\n",
    "                                residualImage.GetGeoTransform(),\n",
    "                                residualImage.GetProjection(),\n",
    "                                f'{os.path.split(outputFilename)[0]}/Residuals/{os.path.split(outputFilename)[1]}_resid{os.path.splitext(outputFilename)[1]}')\n",
    "\n",
    "    outFile = None\n",
    "    residualFile = None\n",
    "    downsaceldFile = None\n",
    "\n",
    "    # print(time.time() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb6550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runEvapi(year, month, day, comp, sharp, s2Mask, lstMask, tile, tempDir, path_to_temp, path_to_sharp, mvwin, cv, regrat, evap_outFolder):\n",
    "\n",
    "    storPath_c = f'{evap_outFolder}{comp}_{year}_{month}_{day}_{mvwin}_{cv}_{regrat}_{lstMask}_{s2Mask}_{sharp}_{tile}_ET_Canopy_calc.tif'\n",
    "    storPath_s = f'{evap_outFolder}{comp}_{year}_{month}_{day}_{mvwin}_{cv}_{regrat}_{lstMask}_{s2Mask}_{sharp}_{tile}_ET_Soil_calc.tif'\n",
    "\n",
    "    storPath_c_f = f'{evap_outFolder}{comp}_{year}_{month}_{day}_{mvwin}_{cv}_{regrat}_{lstMask}_{s2Mask}_{sharp}_{tile}_ET_Canopy_func.tif'\n",
    "    storPath_s_f = f'{evap_outFolder}{comp}_{year}_{month}_{day}_{mvwin}_{cv}_{regrat}_{lstMask}_{s2Mask}_{sharp}_{tile}_ET_Soil_func.tif'\n",
    "    \n",
    "    # path to era5 raw data\n",
    "    era5_path = '/data/Aldhani/eoagritwin/et/Auxiliary/ERA5/grib/'\n",
    "    ssrd_mean_path = '/data/Aldhani/eoagritwin/et/Auxiliary/ERA5/ssrd_mean_calc/'\n",
    "\n",
    "    # the DEM, SLOPE, ASPECT, LAT, LON will be used to sharpen some of the era5 variables (the the resolution of the DEM)\n",
    "    dem_path = '/data/Aldhani/eoagritwin/et/Auxiliary/DEM/reprojected/DEM_GER_FORCE_WARP.tif' # epsg 4326\n",
    "    slope_path = '/data/Aldhani/eoagritwin/et/Auxiliary/DEM/reprojected/SLOPE_GER_FORCE_WARP.tif' # epsg 4326\n",
    "    aspect_path = '/data/Aldhani/eoagritwin/et/Auxiliary/DEM/reprojected/ASPECT_GER_FORCE_WARP.tif' # epsg 4326\n",
    "    lat_path = '/data/Aldhani/eoagritwin/et/Auxiliary/DEM/reprojected/LAT_GER_FORCE_WARP.tif' # epsg 4326\n",
    "    lon_path = '/data/Aldhani/eoagritwin/et/Auxiliary/DEM/reprojected/LON_GER_FORCE_WARP.tif' # epsg 4326\n",
    "\n",
    "    # the geopotential is needed for the sharpening as well\n",
    "    geopot_path = '/data/Aldhani/eoagritwin/et/Auxiliary/ERA5/tiff/low_res/geopotential/geopotential_low_res.tif' # epsg 4326\n",
    "    \n",
    "\n",
    "    # path_base to sharpenend folder and S2_comp\n",
    "    sharp_pathbase = f'{path_to_sharp}Values/'\n",
    "    s2_pathbase = path_to_temp\n",
    "\n",
    "    # the LST acquisition time should determine which sharpened LST files are associatedto be processed (as they are associated with it)\n",
    "    LST_acq_file = f'/data/Aldhani/eoagritwin/et/Sentinel3/LST/LST_values/Acq_time/int_format/{year}/Daily_AcqTime_{comp}_{year}_{month}.tif' # epsg 4326\n",
    "\n",
    "    # the VZA at the time of LST acquisition is need\n",
    "    VZA_at_acq_file = f'/data/Aldhani/eoagritwin/et/Sentinel3/VZA/comp/{comp}/{year}/Daily_VZA_{comp}_{year}_{month}.tif' # epsg 4326\n",
    "\n",
    "    # sharpened LST\n",
    "    LST_file = f'{sharp_pathbase}{comp}_{year}_{month}_{day:02d}_{mvwin}_{cv}_{regrat}_{s2Mask}_{sharp}_{lstMask}_{tile}.tif' \n",
    "    # for NDVI calculation (estimating LAI and others) and warping to S2 resolution, we use the S2 composite used for sharpening\n",
    "    # S2_file = [file for file in getFilelist(s2_pathbase, 'vrt', deep=False) if f'HIGHRES_{comp}_{year}_{month}_{day:02d}' in file][0]\n",
    "    S2_file = [file for file in getFilelist(s2_pathbase, 'vrt', deep=True) if 'S2' in file][0]\n",
    "\n",
    "    # find era5 file that matches the month of LST observation\n",
    "    valid_variables = sorted(list(dict.fromkeys(file.split('/')[-2] for file in getFilelist(era5_path, '.grib', deep=True) \\\n",
    "                                    if not any(var in file for var in ['geopotential', 'total_column_water_vapour']))))\n",
    "\n",
    "    # get a list for those era5 files that match the year and month of the provided LST acquisition file\n",
    "    era5_path_list = find_grib_file(getFilelist(era5_path, '.grib', deep=True), LST_acq_file)\n",
    "    era5_path_list = [path for path in era5_path_list if any(variable in path for variable in valid_variables)] # era5 are epsg 4326 and still will be after warping to doy\n",
    "    temp_pressure_checker(era5_path_list)\n",
    "\n",
    "    # warp datasets needed for calculations to the spatial extent of the sharpened LST\n",
    "    LST_acq_spatial_sub = warp_raster_to_reference(source_path=LST_acq_file, reference_path=S2_file, output_path='MEM', resampling='near')\n",
    "    VZA_at_acq_file_sub = warp_raster_to_reference(source_path=VZA_at_acq_file, reference_path=S2_file, output_path='MEM', resampling='near')\n",
    "    dem_sub = warp_raster_to_reference(source_path=dem_path, reference_path=S2_file, output_path='MEM', resampling='bilinear')\n",
    "    slope_sub  = warp_raster_to_reference(source_path=slope_path, reference_path=S2_file, output_path='MEM', resampling='bilinear')\n",
    "    aspect_sub = warp_raster_to_reference(source_path=aspect_path, reference_path=S2_file, output_path='MEM', resampling='bilinear')\n",
    "    lat_sub = warp_raster_to_reference(source_path=lat_path, reference_path=S2_file, output_path='MEM', resampling='bilinear')\n",
    "    lon_sub = warp_raster_to_reference(source_path=lon_path, reference_path=S2_file, output_path='MEM', resampling='bilinear')\n",
    "    geopot_sub = warp_raster_to_reference(source_path=geopot_path, reference_path=S2_file, output_path='MEM', resampling='bilinear')\n",
    "\n",
    "\n",
    "    # load the era5 variable into cache at LST resolution and read-in the modelled times (one time step per band)\n",
    "    for path in era5_path_list:\n",
    "        # print(f'processing {path}')\n",
    "        # check if DEM sharpener needs to be applied\n",
    "        if '100m_u_component_of_wind' in path:\n",
    "            # do the warping without sharpening\n",
    "            try:\n",
    "                wind100_u = get_warped_ERA5_at_doy(path_to_era_grib=path, reference_path=LST_acq_spatial_sub, lst_acq_file=LST_acq_spatial_sub, doy=day)\n",
    "            except Exception as e:\n",
    "                with open(f'{tempDir}ERROR_{comp}_{year}_{month}_{day}_{mvwin}_{cv}_{regrat}_{lstMask}_{s2Mask}_{sharp}_ET.log', 'a') as f:\n",
    "                    f.write(f'{e}')\n",
    "                return\n",
    "\n",
    "        elif '100m_v_component_of_wind' in path:\n",
    "                # do the warping without sharpening\n",
    "            try:\n",
    "                wind100_v = get_warped_ERA5_at_doy(path_to_era_grib=path, reference_path=LST_acq_spatial_sub, lst_acq_file=LST_acq_spatial_sub, doy=day)\n",
    "            except Exception as e:\n",
    "                with open(f'{tempDir}ERROR_{comp}_{year}_{month}_{day}_{mvwin}_{cv}_{regrat}_{lstMask}_{s2Mask}_{sharp}_ET.log', 'a') as f:\n",
    "                    f.write(f'{e}')\n",
    "                return\n",
    "        # elif 'geopotential' in path:\n",
    "        #     # do the warping without sharpening\n",
    "        #     geopot = get_warped_ERA5_at_doy(path_to_era_grib=path, lst_acq_file=LST_acq_file, doy=day)\n",
    "\n",
    "        elif 'downward' in path: # terrain correction included\n",
    "            try:\n",
    "                ssrd, szenith, sazimuth, ssrd_nc, ssrd_mean_func = get_ssrdsc_warped_and_corrected_at_doy(path_to_ssrdsc_grib=path, reference_path=LST_acq_spatial_sub, \n",
    "                                                                                lst_acq_file=LST_acq_spatial_sub, doy=day, \n",
    "                                                                                slope_path=slope_sub,\n",
    "                                                                                aspect_path=aspect_sub,\n",
    "                                                                                dem_path=dem_sub,\n",
    "                                                                                lat_path=lat_sub,\n",
    "                                                                                lon_path=lon_sub)\n",
    "            except Exception as e:\n",
    "                with open(f'{tempDir}ERROR_{comp}_{year}_{month}_{day}_{mvwin}_{cv}_{regrat}_{lstMask}_{s2Mask}_{sharp}_ET.log', 'a') as f:\n",
    "                    f.write(f'{e}')\n",
    "                return\n",
    "            \n",
    "        elif '2m_temperature' in path: # DEM and adiabatic sharpening, following Guzinski 2021\n",
    "            try:\n",
    "                air_temp = get_warped_ERA5_at_doy(path_to_era_grib=path, reference_path=LST_acq_spatial_sub, \n",
    "                                                lst_acq_file=LST_acq_spatial_sub, doy=day,\n",
    "                                                sharp_blendheight=100,\n",
    "                                                sharp_DEM=dem_sub,\n",
    "                                                sharp_geopot=geopot_sub,\n",
    "                                                sharp_rate=STANDARD_ADIABAT,\n",
    "                                                sharpener='adiabatic')\n",
    "            except Exception as e:\n",
    "                with open(f'{tempDir}ERROR_{comp}_{year}_{month}_{day}_{mvwin}_{cv}_{regrat}_{lstMask}_{s2Mask}_{sharp}_ET.log', 'a') as f:\n",
    "                    f.write(f'{e}')\n",
    "                return\n",
    "            \n",
    "        elif '2m_dewpoint_temperature' in path: # DEM and adiabatic sharpening, following Guzinski 2021\n",
    "            try:\n",
    "                dew_temp = get_warped_ERA5_at_doy(path_to_era_grib=path, reference_path=LST_acq_spatial_sub, \n",
    "                                                lst_acq_file=LST_acq_spatial_sub, doy=day,\n",
    "                                                sharp_blendheight=100,\n",
    "                                                sharp_DEM=dem_sub,\n",
    "                                                sharp_geopot=geopot_sub,\n",
    "                                                sharp_rate=MOIST_ADIABAT,\n",
    "                                                sharpener='adiabatic')\n",
    "            except Exception as e:\n",
    "                with open(f'{tempDir}ERROR_{comp}_{year}_{month}_{day}_{mvwin}_{cv}_{regrat}_{lstMask}_{s2Mask}_{sharp}_ET.log', 'a') as f:\n",
    "                    f.write(f'{e}')\n",
    "                return\n",
    "            \n",
    "        else: \n",
    "            # do warping with DEM sharpening only\n",
    "            # sanity check\n",
    "            if not 'surface_pressure' in path:\n",
    "                raise ValueError('There is and unattended ERA5 variable in the loop - CHECK!!!!')\n",
    "            else:\n",
    "                try:\n",
    "                    sp = get_warped_ERA5_at_doy(path_to_era_grib=path, reference_path=LST_acq_spatial_sub, \n",
    "                                                lst_acq_file=LST_acq_spatial_sub, doy=day,\n",
    "                                                sharp_DEM=dem_sub,\n",
    "                                                sharp_blendheight=100,\n",
    "                                                sharp_geopot=geopot_sub,\n",
    "                                                sharp_temp=air_temp,\n",
    "                                                sharpener='barometric') / 100\n",
    "                except Exception as e:\n",
    "                    with open(f'{tempDir}ERROR_{comp}_{year}_{month}_{day}_{mvwin}_{cv}_{regrat}_{lstMask}_{s2Mask}_{sharp}_ET.log', 'a') as f:\n",
    "                        f.write(f'{e}')\n",
    "                    return\n",
    "                \n",
    "\n",
    "    wind_speed_20 = calc_wind_speed(wind100_u, wind100_v) # check wind_u\n",
    "    \n",
    "\n",
    "    ds = gdal.Open(f'{ssrd_mean_path}surface_solar_radiation_downward_clear_sky_{year}_{int(MONTH_TO_02D[month])}')\n",
    "    ssrd_mean = ds.GetRasterBand(day).ReadAsArray() / 3600\n",
    "    \n",
    "    ssrd_mean_calc_20 = warp_np_to_reference(ssrd_mean, f'{ssrd_mean_path}surface_solar_radiation_downward_clear_sky_{year}_{int(MONTH_TO_02D[month])}', LST_file) # check this too!!!!!\n",
    "    ssrd_mean_func_20 = ssrd_mean_func\n",
    "    ssrd_20 = ssrd\n",
    "    air_temp_20 = air_temp\n",
    "    dew_temp_20 = dew_temp\n",
    "    sp_20 = sp\n",
    "    szenith_20 = szenith\n",
    "    sazimuth_20 = sazimuth\n",
    "\n",
    "    # calculate windspeed\n",
    "\n",
    "    # load vza\n",
    "    vza_ds = VZA_at_acq_file_sub\n",
    "    vza_20 = vza_ds.GetRasterBand(day).ReadAsArray()\n",
    "\n",
    "    # load sharpened LST\n",
    "    lst_ds = gdal.Open(LST_file)\n",
    "    lst_20 =lst_ds.GetRasterBand(1).ReadAsArray()\n",
    "\n",
    "    del wind100_u, wind100_v, ssrd, air_temp, dew_temp, sp, szenith, sazimuth, ssrd_mean, ssrd_nc, ssrd_mean_func\n",
    "\n",
    "\n",
    "    condition = (air_temp_20 > 0) & (dew_temp_20 > 0)  & (sp_20 > 0) & (szenith_20 > 0) & (sazimuth_20 > 0) & (wind_speed_20 > 0) & (lst_20 > 0) & (vza_20 > 0)\n",
    "    ssrd_20[~condition] = np.nan\n",
    "    ssrd_mean_calc_20[~condition] = np.nan\n",
    "    ssrd_mean_func_20[~condition] = np.nan\n",
    "    air_temp_20[~condition] = np.nan\n",
    "    dew_temp_20[~condition] = np.nan\n",
    "    sp_20[~condition] = np.nan\n",
    "    szenith_20[~condition] = np.nan\n",
    "    sazimuth_20[~condition] = np.nan\n",
    "    wind_speed_20[~condition] = np.nan\n",
    "    # lst_20 = np.ma.masked_where(~condition, lst_20)\n",
    "    # vza_20 = np.ma.masked_where(~condition, vza_20)\n",
    "    lst_20[~condition] = np.nan\n",
    "    vza_20[~condition] = np.nan\n",
    "\n",
    "    # calculate the NDVI from the S2 composite (following formula from force --> bandnames: (NIR - RED) / (NIR + RED))\n",
    "    S2_ds = gdal.Open(S2_file)\n",
    "    for idx, bname in enumerate(getBandNames(S2_file)):\n",
    "        if bname == 'RED':\n",
    "            red = S2_ds.GetRasterBand(1 + idx).ReadAsArray()\n",
    "        elif bname == 'NIR':\n",
    "            nir = S2_ds.GetRasterBand(1 + idx).ReadAsArray()\n",
    "        else:\n",
    "            continue\n",
    "    ndvi_20 = (nir - red) / (nir + red)\n",
    "    ndvi_20_ma = np.where(ndvi_20 < 0, np.nan, ndvi_20)\n",
    "    # ndvi_20_ma = np.ma.masked_invalid(ndvi_20)\n",
    "    # ndvi_20_ma = np.ma.masked_where(ndvi_20_ma < 0, ndvi_20_ma)\n",
    "    LAI_np = 0.57*np.exp(2.33*ndvi_20)\n",
    "    LAI_pos = np.where(LAI_np < 0, np.nan, LAI_np)\n",
    "\n",
    "    # estimate canopy height from estimated LAI\n",
    "    hc = hc_from_lai(LAI_pos, hc_max = 1.2, lai_max = np.nanmax(LAI_np), hc_min=0)\n",
    "\n",
    "    # estimate long wave irradiance\n",
    "    ea = meteo_utils.calc_vapor_pressure(T_K=dew_temp_20)\n",
    "    L_dn = calc_longwave_irradiance(ea = ea, t_a_k = air_temp_20, p = sp_20, z_T = 100, h_C = hc) # ## does that make sense with the 100m!!!!!!!!!!!!!!!!!!!\n",
    "    d_0_0 = resistances.calc_d_0(h_C=hc)\n",
    "    z_0 = resistances.calc_z_0M(h_C=hc)\n",
    "\n",
    "\n",
    "    # calculate shortwave radiation of soil and canopy\n",
    "    difvis, difnir, fvis, fnir = net_radiation.calc_difuse_ratio(S_dn = ssrd_20, sza = np.nanmean(szenith_20))\n",
    "\n",
    "    skyl = difvis * fvis + difnir * fnir\n",
    "    S_dn_dir = ssrd_20 * (1.0 - skyl)\n",
    "    S_dn_dif = ssrd_20 * skyl\n",
    "\n",
    "    # Leaf spectral properties:{rho_vis_C: visible reflectance, tau_vis_C: visible transmittance, rho_nir_C: NIR reflectance, tau_nir_C: NIR transmittance}\n",
    "    rho_vis_C=np.full(LAI_pos.shape, 0.05, np.float32)\n",
    "    tau_vis_C=np.full(LAI_pos.shape, 0.08, np.float32)\n",
    "    rho_nir_C=np.full(LAI_pos.shape, 0.32, np.float32)\n",
    "    tau_nir_C=np.full(LAI_pos.shape, 0.33, np.float32) \n",
    "\n",
    "    # Soil spectral properties:{rho_vis_S: visible reflectance, rho_nir_S: NIR reflectance}\n",
    "    rho_vis_S=np.full(LAI_pos.shape, 0.07, np.float32)\n",
    "    rho_nir_S=np.full(LAI_pos.shape, 0.25, np.float32)\n",
    "\n",
    "    # F = local LAI\n",
    "    F = LAI_pos / 1\n",
    "    # calculate clumping index\n",
    "    Omega0 = clumping_index.calc_omega0_Kustas(LAI = LAI_np, f_C = 1, x_LAD=1)\n",
    "    Omega = clumping_index.calc_omega_Kustas(Omega0, np.nanmean(szenith_20))\n",
    "    LAI_eff = F * Omega\n",
    "\n",
    "    Sn_C, Sn_S = net_radiation.calc_Sn_Campbell(lai = LAI_pos, sza = np.mean(szenith_20), S_dn_dir = S_dn_dir, S_dn_dif = S_dn_dif, fvis = fvis,\n",
    "                                        fnir = fnir, rho_leaf_vis = rho_vis_C, tau_leaf_vis = tau_vis_C, rho_leaf_nir = rho_nir_C, \n",
    "                                        tau_leaf_nir = tau_nir_C, rsoilv = rho_vis_S, rsoiln = rho_nir_S, x_LAD=1, LAI_eff=LAI_eff)\n",
    "\n",
    "    # calculate other roughness stuff\n",
    "    z_0M, d = resistances.calc_roughness(LAI=np.nanmean(LAI_pos), h_C=hc, w_C=1, landcover=11, f_c=None)\n",
    "    fg = calc_fg_gutman(ndvi = ndvi_20_ma, ndvi_min = np.nanmin(ndvi_20), ndvi_max = np.nanmax(ndvi_20))\n",
    "\n",
    "    emis_C = 0.98\n",
    "    emis_S = 0.95\n",
    "    h_C = hc \n",
    "    z_u = 100\n",
    "    z_T = 100\n",
    "\n",
    "    output = TSEB.TSEB_PT(lst_20, vza_20, air_temp_20, wind_speed_20, ea, sp_20, Sn_C, Sn_S, L_dn, LAI_pos, h_C, emis_C, emis_S, \n",
    "    z_0M, d, z_u, z_T, resistance_form=None, calcG_params=None, const_L=None, \n",
    "    kB=0.0, massman_profile=None, verbose=True)\n",
    "\n",
    "    for stori, ssrd_ras in zip([[storPath_c, storPath_s],[storPath_c_f, storPath_s_f]], [ssrd_mean_calc_20, ssrd_mean_func_20]):\n",
    "        \n",
    "        le_c = output[6]/ssrd_20\n",
    "        heat_latent_scaled_c = ssrd_ras * le_c\n",
    "        et_daily_c = TSEB.met.flux_2_evaporation(heat_latent_scaled_c, t_k=air_temp_20, time_domain=24)\n",
    "\n",
    "        le_s = output[8]/ssrd_20\n",
    "        heat_latent_scaled_s = ssrd_ras * le_s\n",
    "        et_daily_s = TSEB.met.flux_2_evaporation(heat_latent_scaled_s, t_k=air_temp_20, time_domain=24)\n",
    "\n",
    "        npTOdisk(et_daily_c, LST_file, stori[0])\n",
    "        npTOdisk(et_daily_s, LST_file, stori[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "221a4827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "lowmask_path = '/data/Aldhani/eoagritwin/et/Auxiliary/DEM/reprojected/THUENEN_GER_LST_WARP.tif'\n",
    "lowmask_bin_path = '/data/Aldhani/eoagritwin/et/Auxiliary/DEM/reprojected/THUENEN_GER_LST_WARP_BINARY.tif'\n",
    "temp_dump = '/data/Aldhani/eoagritwin/et/Sentinel3/LST/SharpEvap/Brandenburg/'\n",
    "trash_path = f'{temp_dump}trash/'\n",
    "\n",
    "path_to_slope = '/data/Aldhani/eoagritwin/et/Auxiliary/DEM/Force_Tiles/SLOPE/'\n",
    "path_to_aspect = '/data/Aldhani/eoagritwin/et/Auxiliary/DEM/Force_Tiles/ASPECT/'\n",
    "path_to_agro = '/data/Aldhani/eoagritwin/et/Auxiliary/DEM/Force_Tiles/THUENEN_2021/'\n",
    "\n",
    "path_to_force = '/data/Aldhani/eoagritwin/force/output/Guzinski'\n",
    "path_to_inci = '/data/Aldhani/eoagritwin/et/Auxiliary/DEM/Force_Tiles/INCIDENCE2/'\n",
    "path_to_lst = '/data/Aldhani/eoagritwin/et/Sentinel3/LST/LST_values/LST_composites/'\n",
    "\n",
    "# get the Tiles for Brandenburg\n",
    "bran = pd.read_csv('/data/Aldhani/eoagritwin/misc/state_tile_csv/clipped_grid_bran_tiles.csv')\n",
    "tiles_to_process = createFORCEtileLIST(list(bran['Tile_X']),\n",
    "                                        list(bran['Tile_Y']), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c372f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sharp_Evap(tile_to_process, storFolder, path_to_slope, path_to_aspect, path_to_agro, path_to_force, time_start, time_end, compList, predList, S2mask):\n",
    "\n",
    "    temp_dump_fold = f\"{storFolder}temp/{tile_to_process.replace('_', '')}/\"\n",
    "    sharp_outFolder = f'{storFolder}sharpened/{tile_to_process.replace('_', '')}/'\n",
    "    evap_outFolder = f'{storFolder}evap/{tile_to_process.replace('_', '')}/'\n",
    "\n",
    "    for foldi in [temp_dump_fold, sharp_outFolder, evap_outFolder]:\n",
    "        if not os.path.exists(foldi):\n",
    "            os.makedirs(foldi,exist_ok=False)\n",
    "\n",
    "    year = time_start[:4]\n",
    "\n",
    "    # ############## make vrts for slope, aspect and agromask\n",
    "    slopes = [file for file in getFilelist(path_to_slope, '.tif') if tile_to_process in file] # if any tile name is in file\n",
    "    # aspect-tiles\n",
    "    aspects = [file for file in getFilelist(path_to_aspect, '.tif') if tile_to_process in file] # if any tile name is in file\n",
    "    # thuenen-tiles\n",
    "    thuenen = [file for file in getFilelist(path_to_agro, '.tif') if tile_to_process in file] # if any tile name is in file\n",
    "\n",
    "    # get those tiles (and composite if more than one tile is provided)\n",
    "    slope_path = f'{temp_dump_fold}SLOPE.vrt'\n",
    "    gdal.BuildVRT(slope_path, slopes)\n",
    "\n",
    "    aspect_path = f'{temp_dump_fold}ASPECT.vrt'\n",
    "    gdal.BuildVRT(aspect_path, aspects)\n",
    "\n",
    "    thuenen_path = f'{temp_dump_fold}THUENEN.vrt'\n",
    "    gdal.BuildVRT(thuenen_path, thuenen)\n",
    "\n",
    "    # ################ load force and vrt\n",
    "    path_to_S2_tiles = f'{path_to_force}/{year}/'\n",
    "    # get a list with all available tiles\n",
    "    files = getFilelist(f'{path_to_S2_tiles}/tiles', '.tif', deep=True) \n",
    "    files = [file for file in files if any(tile in file for tile in tiles_to_process)]\n",
    "    date_list = check_forceTSI_compositionDates(files)\n",
    "\n",
    "    # make the mask ready for S2 masking\n",
    "    th_ds = gdal.Open(thuenen_path)\n",
    "    th_arr = th_ds.GetRasterBand(1).ReadAsArray()\n",
    "    mask = np.where(th_arr == -9999, 0, 1)\n",
    "\n",
    "    # needed lists\n",
    "    lowRes_files = []\n",
    "    highRes_files = []\n",
    "    highRes_names = []\n",
    "\n",
    "    colors = ['BLU', 'GRN', 'RED', 'NIR', 'RE1', 'RE2', 'RE3',  'SW1', 'SW2']\n",
    "\n",
    "    # this should be the masterloop within the sharpend, evaping and deleting of all files but the above created vrts takes place\n",
    "    for date in date_list:\n",
    "        if int(time_start) <= int(date) <= int(time_end):\n",
    "            \n",
    "            tilesS2 = [file for file in getFilelist(path_to_S2_tiles, '.tif', deep=True) if tile_to_process in file and f'{date}.tif' in file]\n",
    "            tilesS2 = [t2 for col in colors for t2 in tilesS2 if col in t2]\n",
    "            S2_path = f'{temp_dump_fold}S2_{date}.vrt'\n",
    "            print(S2_path)\n",
    "            vrt = gdal.BuildVRT(S2_path, tilesS2, separate=True)\n",
    "            vrt = None\n",
    "            vrt = gdal.Open(S2_path, gdal.GA_Update)  # VRT must be writable\n",
    "            for idz, bname in enumerate(colors): \n",
    "                band = vrt.GetRasterBand(1+idz)\n",
    "                band.SetDescription(bname)\n",
    "            vrt = None\n",
    "\n",
    "            # determine LST and incidence files associated with respective S2 composite\n",
    "            band_dict = transform_compositeDate_into_LSTbands(date, 4)\n",
    "\n",
    "\n",
    "            # stat used for compositing\n",
    "            for comp_stat in compList: #  \n",
    "                path_to_incident = f'{path_to_inci}{comp_stat}/{year}/'\n",
    "                path_to_LST = f'{path_to_lst}{comp_stat}/{year}/'\n",
    "\n",
    "                # get all LST bands that can be sharped with the S2 composite at this date (and sun angle incidence files as well, as they are dependent on that date\n",
    "                LSTs = []\n",
    "\n",
    "                for k, v in band_dict.items():\n",
    "                    month = v['month']\n",
    "                    band = int(v['band'])\n",
    "                    v_path = f'{path_to_LST}Daily_LST_{comp_stat}_{year}_{month}.tif'\n",
    "                    ds = gdal.Open(v_path, 0)\n",
    "\n",
    "                    # export the LST for that day\n",
    "                    LST_arr = ds.GetRasterBand(band).ReadAsArray() # store as single Tiff in temp\n",
    "                    daily_lst_path = f'{temp_dump_fold}Daily_LST_{comp_stat}_{year}_{month}_{band:02d}.tif'\n",
    "                    makeTif_np_to_matching_tif(LST_arr, v_path, daily_lst_path)\n",
    "                \n",
    "                    # store the paths for selecting incidence for corresponding LST\n",
    "                    incid_date = f'{year}_{month}_{band:02d}.tif'\n",
    "\n",
    "                    # incidence-tiles\n",
    "                    incids = [file for file in getFilelist(path_to_incident, '.tif', deep=True) if tile_to_process in file] \n",
    "                    incid_path = incids[0]\n",
    "\n",
    "                    # create highRes file through exapnding the vrt of S2\n",
    "                    highRes_path = f'{temp_dump_fold}HIGHRES_{comp_stat}_{incid_date.split('.')[0]}.vrt'\n",
    "                    gdal.BuildVRT(highRes_path, [S2_path, slope_path, aspect_path, incid_path], separate=True)\n",
    "\n",
    "                    for predi in predList:\n",
    "                        if predi == 'allpred':\n",
    "                            maskVRT_water(highRes_path)\n",
    "                        else:\n",
    "                            maskVRT_water_and_drop_aux(highRes_path)\n",
    "\n",
    "                        if S2mask == 1:\n",
    "                            highRes_files.append(f'{highRes_path.split('.')[0]}_watermask.tif')\n",
    "                            highRes_names.append(f'S2notMasked_{predi}')\n",
    "                            lowRes_files.append(daily_lst_path)\n",
    "\n",
    "                        elif S2mask == 2:\n",
    "                            maskVRT(f'{highRes_path.split('.')[0]}_watermask.tif', mask, suffix=f'_S2_agromask_{predi}')\n",
    "                            os.remove(f'{highRes_path.split('.')[0]}_watermask.tif')\n",
    "                            highRes_files.append(f'{highRes_path.split('.')[0]}_watermask_S2_agromask_{predi}.tif')\n",
    "                            lowRes_files.append(daily_lst_path)\n",
    "                            highRes_names.append(f'S2Masked_{predi}')\n",
    "\n",
    "                        elif S2mask == 3:\n",
    "                            highRes_files.append(f'{highRes_path.split('.')[0]}_watermask.tif')\n",
    "                            highRes_names.append(f'S2notMasked_{predi}')\n",
    "                            lowRes_files.append(daily_lst_path)\n",
    "                            maskVRT(f'{highRes_path.split('.')[0]}_watermask.tif', mask, suffix=f'_S2_agromask_{predi}')\n",
    "                            highRes_files.append(f'{highRes_path.split('.')[0]}_watermask_S2_agromask_{predi}.tif')\n",
    "                            lowRes_files.append(daily_lst_path)\n",
    "                            highRes_names.append(f'S2Masked_{predi}')\n",
    "            sharpList = []\n",
    "\n",
    "            for idx, highResFilename in enumerate(highRes_files):\n",
    "                    lowResFilename = lowRes_files[idx]\n",
    "                    # f1 = f'{sharp_outFolder}{'/'.join(highResFilename.split('.')[0].split('_')[2:5])}/'\n",
    "                    for maskname, mask_lowRes in zip(['withoutLSTmask'], ['']): # 'withLSTmask'  lowmask_bin_path\n",
    "                    # for maskname, mask_lowRes in zip(['withoutLSTmask', 'withLSTmask'], ['', lowmask_bin_path]):\n",
    "                        lowResMaskFilename = mask_lowRes\n",
    "                        # f2 = f'{f1}{maskname}/'\n",
    "                        for movWin in [15]:\n",
    "                            for cv in [0]:\n",
    "                                for regrat in [0.25]:\n",
    "                                    kombi = f'mvwin{movWin}_cv{cv}_regrat{int(regrat*100):02d}_{highRes_names[idx]}_{maskname}'\n",
    "                                    # f3 = f'{f2}{highRes_names[idx]}/'\n",
    "                                    # os.makedirs(f'{f3}Residuals/', exist_ok=True)\n",
    "                                    # os.makedirs(f'{f3}Values/', exist_ok=True)\n",
    "\n",
    "                                    os.makedirs(f'{sharp_outFolder}Residuals/', exist_ok=True)\n",
    "                                    os.makedirs(f'{sharp_outFolder}Values/', exist_ok=True)\n",
    "                                    \n",
    "                                    # sharpened_file = f'{f3}{'_'.join(highResFilename.split('.')[0].split('_')[2:6])}_{kombi}_{tile_to_process}.tif'\n",
    "                                    sharpened_file = f'{sharp_outFolder}{'_'.join(highResFilename.split('.')[0].split('_')[1:5])}_{kombi}_{tile_to_process}.tif'\n",
    "                                    \n",
    "                                    runSharpi(highResFilename, lowResFilename, lowResMaskFilename, cv, movWin, regrat, sharpened_file, useDecisionTree = True)\n",
    "\n",
    "                                    sharpList.append(sharpened_file)\n",
    "           \n",
    "            for sharped in sharpList:\n",
    "                comp = sharped.split('/')[-1].split('_')[0]\n",
    "                year = sharped.split('/')[-1].split('_')[1]\n",
    "                month = sharped.split('/')[-1].split('_')[2]\n",
    "                day = int(sharped.split('/')[-1].split('_')[3])\n",
    "                mvwin = sharped.split('/')[-1].split('_')[4]\n",
    "                cv = sharped.split('/')[-1].split('_')[5]\n",
    "                regrat = sharped.split('/')[-1].split('_')[6]\n",
    "                sharp = sharped.split('/')[-1].split('_')[8]\n",
    "                s2Mask = sharped.split('/')[-1].split('_')[7]\n",
    "                lstMask = sharped.split('/')[-1].split('_')[9]\n",
    "                tile = '_'.join(sharped.split('/')[-1].split('.')[0].split('_')[-2:])\n",
    "\n",
    "                runEvapi(year=year, month=month, day=day, comp=comp, sharp=sharp, s2Mask=s2Mask, lstMask=lstMask, tile=tile,\n",
    "                        tempDir=trash_path, path_to_temp=temp_dump_fold, path_to_sharp=sharp_outFolder,\n",
    "                        mvwin=mvwin, cv=cv, regrat=regrat, evap_outFolder=evap_outFolder)\n",
    "            \n",
    "                \n",
    "            # at the end of the date loop --> clean up temp folder and sharp\n",
    "            temp_files_vrt = [file for file in getFilelist(temp_dump_fold, '.vrt')]\n",
    "            temp_files_tif = [file for file in getFilelist(temp_dump_fold, '.tif')]\n",
    "            [temp_files_vrt.remove(path) for path in [slope_path, aspect_path, thuenen_path]]\n",
    "            sharp_files = [file for file in getFilelist(sharp_outFolder, '.tif', deep=True)]\n",
    "            [os.remove(file) for file in temp_files_vrt]\n",
    "            [os.remove(file) for file in temp_files_tif]\n",
    "            [os.remove(file) for file in sharp_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sharp_Evap(tile_to_process=tiles_to_process[0], storFolder=temp_dump, path_to_slope=path_to_slope,\n",
    "           path_to_aspect=path_to_aspect, path_to_agro=path_to_agro, path_to_force=path_to_force,\n",
    "           time_start='20190501', time_end='20190701', compList=['maxLST'], predList=['S2only'], S2mask=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c43a3ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_to_process = tiles_to_process[0]\n",
    "storFolder = temp_dump\n",
    "path_to_slope = path_to_slope\n",
    "path_to_aspect = path_to_aspect\n",
    "path_to_agro = path_to_agro\n",
    "path_to_force = path_to_force\n",
    "time_start = '20190328'\n",
    "time_end = '20190701'\n",
    "compList = ['maxLST']\n",
    "predList = ['S2only']\n",
    "S2mask = 2 # 1 = no mask, 2 = S2 mask, 3 = both "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cds_era5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
