{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa91676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/potzschf/repos/')\n",
    "from helperToolz.helpsters import *\n",
    "from helperToolz.dicts_and_lists import *\n",
    "from helperToolz.guzinski import * \n",
    "import geopandas as gpd\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import datetime, timedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bc67cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = 'minVZA'\n",
    "pathi = '/data/Aldhani/eoagritwin/et/Sentinel3/LST/SharpEvap/Brandenburg/FirstShot/evap/'\n",
    "evap_outFolder = f\"{pathi}{comp}/\"\n",
    "vrt_folder= path_safe(f\"{evap_outFolder}vrt2/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e03e935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate for 4 different evap products (soil vs canopy, ssrd calc vs ssrd func)\n",
    "all_files = getFilelist(evap_outFolder, '.tif', deep=True)\n",
    "\n",
    "soil_func_files = [file for file in all_files if 'Soil_func' in file]\n",
    "soil_calc_files = [file for file in all_files if 'Soil_calc' in file]\n",
    "\n",
    "canopy_func_files = [file for file in all_files if 'Canopy_func' in file]\n",
    "canopy_calc_files = [file for file in all_files if 'Canopy_calc' in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1398667b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Aldhani/users/potzschf/conda/envs/workhorse/lib/python3.12/site-packages/osgeo/gdal.py:311: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# get the max extent for vrts. this is based on the mask (which actually was dervied from a vrt created and then deleted as there are some with smaller extent)\n",
    "mask_ds = gdal.Open('/data/Aldhani/eoagritwin/fields/Auxiliary/grid_search/Brandenburg/quick_n_dirty/Fields_as_mask_pixel_20.tif')\n",
    "gt = mask_ds.GetGeoTransform()\n",
    "mask_proj = mask_ds.GetProjection()\n",
    "xmin = gt[0]\n",
    "ymax = gt[3]\n",
    "px_size_x = gt[1]\n",
    "px_size_y = abs(gt[5])\n",
    "xres = mask_ds.RasterXSize\n",
    "yres = mask_ds.RasterYSize\n",
    "xmax = xmin + xres * px_size_x\n",
    "ymin = ymax - yres * px_size_y\n",
    "\n",
    "\n",
    "vrt_options = gdal.BuildVRTOptions(\n",
    "    outputBounds=[xmin, ymin, xmax, ymax],\n",
    "    xRes=px_size_x,       \n",
    "    yRes=px_size_y, \n",
    "    outputSRS=mask_proj,\n",
    "    resampleAlg='nearest',\n",
    "    separate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b67a96df",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = [REAL_INT_TO_MONTH[m] for m in range(1,13) if growingSeasonChecker(m)]\n",
    "\n",
    "date_reg = re.compile(r'20\\d{2}_(?:' + '|'.join(months) + r')_\\d{1,2}_')\n",
    "date_reg2 = re.compile(r'20\\d{2}_(?:' + '|'.join(months) + r')_\\d{1,2}.')\n",
    "\n",
    "filesList = [soil_func_files, soil_calc_files, canopy_func_files, canopy_calc_files]\n",
    "filesNames = ['Soil_func', 'Soil_calc', 'Canopy_func', 'Canopy_calc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0803f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create daily vrts mosaics for each type of evap output (soil, canopy)\n",
    "for fname, fileL in zip(filesNames, filesList):\n",
    "    dicti = defaultdict(list)\n",
    "    for file in fileL:\n",
    "        match = date_reg.search(file)\n",
    "        if match:\n",
    "            date = match.group()[:-1]  \n",
    "            year, month, day = date.split('_')\n",
    "            day = day.zfill(2)  # \"1\" → \"01\", \"13\" → \"13\"\n",
    "            date_key = f\"{year}_{month}_{day}\"  \n",
    "            dicti[date_key].append(file)\n",
    "            continue\n",
    "    \n",
    "    for key, files in dicti.items():\n",
    "        \n",
    "        outfolder = f'{vrt_folder}{fname}/'\n",
    "\n",
    "        vrt_path = path_safe(f'{outfolder}{fname}_{key}.vrt')\n",
    "        vrt = gdal.BuildVRT(vrt_path, files, options=vrt_options)\n",
    "        vrt = None\n",
    "\n",
    "        convertVRTpathsTOrelative(vrt_path=vrt_path)\n",
    "        pass\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85ba2d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the comp dates\n",
    "compDates = pd.read_csv(getFilelist(pathi, '.csv', deep=True)[0])['compdates'].tolist()\n",
    "\n",
    "starts = [datetime.strptime(str(compDate), '%Y%m%d') - timedelta(days=4) for compDate in compDates]\n",
    "ends = [datetime.strptime(str(compDate), '%Y%m%d') + timedelta(days=4) for compDate in compDates]\n",
    "\n",
    "# check lowest and highest start date in vrts and adapt start and end if needed\n",
    "vrt_dates = list(dicti.keys())\n",
    "vrt_datesL = []\n",
    "for vrt_date in vrt_dates:\n",
    "    year, month, day = vrt_date.split('_') \n",
    "    vrt_datesL.append(f'{year}{MONTH_TO_02D[month]}{day}')\n",
    "vrt_start = datetime.strptime(str(min(vrt_datesL)), '%Y%m%d')\n",
    "vrt_end = datetime.strptime(str(max(vrt_datesL)), '%Y%m%d')\n",
    "\n",
    "if vrt_start < starts[0]:\n",
    "    starts[0] = vrt_start\n",
    "\n",
    "if vrt_end > ends[-1]:\n",
    "    ends[-1] = vrt_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd03d5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mask for vrts (based on the polygonized delineated fields (@20m!!!!))\n",
    "mask_arr = mask_ds.GetRasterBand(1).ReadAsArray()\n",
    "mask_arr[mask_arr>0] = 1\n",
    "\n",
    "outpath_single = path_safe(f\"{evap_outFolder}median_9day2/ET_single/\")\n",
    "outpath_sum = path_safe(f\"{evap_outFolder}median_9day2/ET_sum/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8faa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ET_median(vrt_folder, ET_var, date_s, date_e, reg_search, outFolder, mask=False):\n",
    "    \n",
    "    files = getFilelist(f'{vrt_folder}{ET_var}/', '.vrt')\n",
    "    dicti = defaultdict(list)\n",
    "\n",
    "    for file in files:\n",
    "            m = reg_search.search(file).group()[:-1]\n",
    "            year, month, day = m.split('_')\n",
    "            day = day.zfill(2)  # \"1\" → \"01\", \"13\" → \"13\"\n",
    "            date_key = f\"{year}{MONTH_TO_02D[month]}{day}\"\n",
    "            \n",
    "            if date_s <= datetime.strptime(date_key, '%Y%m%d') < date_e:\n",
    "                dicti[datetime.strftime(date_s, '%Y_%m_%d')].append(file)\n",
    "        \n",
    "    for k, v in dicti.items():\n",
    "        arrL = []\n",
    "        for file in v:\n",
    "            ds = gdal.Open(file)\n",
    "            arr = ds.GetRasterBand(1).ReadAsArray()\n",
    "            arr[arr<=0] = np.nan\n",
    "            arr[arr>12] = np.nan\n",
    "            arrL.append(arr)\n",
    "        median_arr = np.nanmedian(np.dstack(arrL),axis=2)\n",
    "\n",
    "        if mask is not False:\n",
    "             outarr = median_arr * mask\n",
    "        else:\n",
    "             outarr = median_arr\n",
    "        makeTif_np_to_matching_tif(outarr, file, f\"{outFolder}{ET_var}_{k}_median_ET.tif\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblist = []\n",
    "\n",
    "for et_var in ['Soil_func', 'Soil_calc', 'Canopy_func', 'Canopy_calc']:\n",
    "    for date_start, date_end in zip(starts, ends):\n",
    "        joblist.append([vrt_folder, et_var, date_start, date_end, date_reg2, outpath_single, mask_arr])\n",
    "\n",
    "\n",
    "print(f'\\n{len(joblist)} jobs will be processed\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6601952",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncores = 4\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    starttime = time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Starting process, time:\" + starttime)\n",
    "    print(\"\")\n",
    "\n",
    "    Parallel(n_jobs=ncores)(delayed(make_ET_median)(job[0], job[1], job[2], job[3], job[4], job[5], job[6]) for job in joblist)\n",
    "\n",
    "    print(\"\")\n",
    "    endtime = time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"start : \" + starttime)\n",
    "    print(\"end: \" + endtime)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e86e1f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "medList = getFilelist(outpath_single, '.tif')\n",
    "for date_start in starts:\n",
    "\n",
    "    dt = datetime.strftime(date_start, '%Y_%m_%d')\n",
    "    date_sub_calc = [med_arr for med_arr in medList if dt in med_arr and 'calc' in med_arr]\n",
    "    date_sub_func = [med_arr for med_arr in medList if dt in med_arr and 'func' in med_arr]\n",
    "\n",
    "    arrL = []\n",
    "    for pathi in date_sub_calc:\n",
    "        ds = gdal.Open(pathi)\n",
    "        arrL.append(ds.GetRasterBand(1).ReadAsArray())\n",
    "    arr_sum = np.nansum(np.dstack([arrL[0], arrL[1]]),axis=2)\n",
    "    arr_sum[arr_sum >= 10] = np.nan\n",
    "    # we cut off the highest values at the Polish border for now\n",
    "    cutOff = np.nanpercentile(arr_sum, [99.9])[0]\n",
    "    arr_sum[arr_sum > cutOff] = np.nan\n",
    "    makeTif_np_to_matching_tif(arr_sum, pathi, f\"{outpath_sum}{dt}_median_ET.tif\", 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
