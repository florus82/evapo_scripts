{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "241a3a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/potzschf/repos/')\n",
    "from helperToolz.helpsters import *\n",
    "from helperToolz.dicts_and_lists import *\n",
    "from helperToolz.guzinski import * \n",
    "import geopandas as gpd\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import datetime, timedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a28e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = 'minVZA'\n",
    "pathi = '/data/Aldhani/eoagritwin/et/Sentinel3/LST/SharpEvap/Brandenburg/FirstShot/evap/'\n",
    "evap_outFolder = f'{pathi}{comp}/'\n",
    "vrt_folder= f'{evap_outFolder}vrt/'\n",
    "\n",
    "if not os.path.exists(vrt_folder):\n",
    "    os.makedirs(vrt_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83aef5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate for 4 different evap products (soil vs canopy, ssrd calc vs ssrd func)\n",
    "all_files = getFilelist(evap_outFolder, '.tif', deep=True)\n",
    "\n",
    "soil_func_files = [file for file in all_files if 'Soil_func' in file]\n",
    "soil_calc_files = [file for file in all_files if 'Soil_calc' in file]\n",
    "\n",
    "canopy_func_files = [file for file in all_files if 'Canopy_func' in file]\n",
    "canopy_calc_files = [file for file in all_files if 'Canopy_calc' in file]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83da55df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the max extent for vrts. this is based on the mask (which actually was dervied from a vrt created and then deleted as there are some with smaller extent)\n",
    "mask_ds = gdal.Open('/data/Aldhani/eoagritwin/fields/Auxiliary/grid_search/Brandenburg/quick_n_dirty/Fields_as_mask_pixel_20.tif')\n",
    "gt = mask_ds.GetGeoTransform()\n",
    "mask_proj = mask_ds.GetProjection()\n",
    "xmin = gt[0]\n",
    "ymax = gt[3]\n",
    "px_size_x = gt[1]\n",
    "px_size_y = abs(gt[5])\n",
    "xres = mask_ds.RasterXSize\n",
    "yres = mask_ds.RasterYSize\n",
    "xmax = xmin + xres * px_size_x\n",
    "ymin = ymax - yres * px_size_y\n",
    "\n",
    "\n",
    "vrt_options = gdal.BuildVRTOptions(\n",
    "    outputBounds=[xmin, ymin, xmax, ymax],\n",
    "    xRes=px_size_x,       \n",
    "    yRes=px_size_y, \n",
    "    outputSRS=mask_proj,\n",
    "    resampleAlg='nearest',\n",
    "    separate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39e1c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['April', 'May', 'June', 'July', 'August', 'September', 'October']\n",
    "\n",
    "date_reg = re.compile(r'20\\d{2}_(?:' + '|'.join(months) + r')_\\d{1,2}_')\n",
    "date_reg2 = re.compile(r'20\\d{2}_(?:' + '|'.join(months) + r')_\\d{1,2}.')\n",
    "\n",
    "filesList = [soil_func_files, soil_calc_files, canopy_func_files, canopy_calc_files]\n",
    "filesNames = ['Soil_func', 'Soil_calc', 'Canopy_func', 'Canopy_calc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2082946",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = filesNames[0]\n",
    "fileL =filesList[0]\n",
    "dicti = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b169db4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019_April_10'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_reg.search(fileL[0]).group()[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "332cabb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['April', 'May', 'June', 'July', 'August', 'September', 'October']\n",
    "\n",
    "date_reg = re.compile(r'20\\d{2}_(?:' + '|'.join(months) + r')_\\d{1,2}_')\n",
    "date_reg2 = re.compile(r'20\\d{2}_(?:' + '|'.join(months) + r')_\\d{1,2}.')\n",
    "\n",
    "filesList = [soil_func_files, soil_calc_files, canopy_func_files, canopy_calc_files]\n",
    "filesNames = ['Soil_func', 'Soil_calc', 'Canopy_func', 'Canopy_calc']\n",
    "\n",
    "for fname, fileL in zip(filesNames, filesList):\n",
    "    dicti = defaultdict(list)\n",
    "    for file in fileL:\n",
    "        match = date_reg.search(file)\n",
    "        if match:\n",
    "            date = match.group()[:-1]  \n",
    "            year, month, day = date.split('_')\n",
    "            day = day.zfill(2)  # \"1\" → \"01\", \"13\" → \"13\"\n",
    "            date_key = f\"{year}_{month}_{day}\"  \n",
    "            dicti[date_key].append(file)\n",
    "            continue\n",
    "    \n",
    "    for key, files in dicti.items():\n",
    "        \n",
    "        # outfolder = f'{vrt_folder}{fname}/'\n",
    "\n",
    "        # vrt_path = path_safe(f'{outfolder}{fname}_{key}.vrt')\n",
    "        # vrt = gdal.BuildVRT(vrt_path, files, options=vrt_options)\n",
    "        # vrt = None\n",
    "\n",
    "        # convertVRTpathsTOrelative(vrt_path=vrt_path)\n",
    "        pass\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b53d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the comp dates incorporate a sanity check maybe???\n",
    "\n",
    "compDates = pd.read_csv(getFilelist(pathi, '.csv', deep=True)[0])['compdates'].tolist()\n",
    "\n",
    "starts = [datetime.strptime(str(compDate), '%Y%m%d') - timedelta(days=4) for compDate in compDates]\n",
    "ends = [datetime.strptime(str(compDate), '%Y%m%d') + timedelta(days=4) for compDate in compDates]\n",
    "\n",
    "# check lowest and highest start date in vrts and adapt start and end if needed\n",
    "vrt_dates = list(dicti.keys())\n",
    "vrt_datesL = []\n",
    "for vrt_date in vrt_dates:\n",
    "    year, month, day = vrt_date.split('_') \n",
    "    vrt_datesL.append(f'{year}{MONTH_TO_02D[month]}{day}')\n",
    "vrt_start = datetime.strptime(str(min(vrt_datesL)), '%Y%m%d')\n",
    "vrt_end = datetime.strptime(str(max(vrt_datesL)), '%Y%m%d')\n",
    "\n",
    "if vrt_start < starts[0]:\n",
    "    starts[0] = vrt_start\n",
    "\n",
    "if vrt_end > ends[-1]:\n",
    "    ends[-1] = vrt_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c192183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask the vrts\n",
    "mask_arr = mask_ds.GetRasterBand(1).ReadAsArray()\n",
    "mask_arr[mask_arr>0] = 1\n",
    "\n",
    "os.makedirs(f'{evap_outFolder}median_9day/', exist_ok=True)\n",
    "os.makedirs(f'{evap_outFolder}median_9day/ET_sum/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a4ae67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ET_median(vrt_folder, ET_var, date_s, date_e, reg_search, outFold, mask=False):\n",
    "    \n",
    "    files = getFilelist(f'{vrt_folder}{ET_var}/', '.vrt')\n",
    "    dicti = defaultdict(list)\n",
    "\n",
    "    for file in files:\n",
    "            m = reg_search.search(file).group()[:-1]\n",
    "            year, month, day = m.split('_')\n",
    "            day = day.zfill(2)  # \"1\" → \"01\", \"13\" → \"13\"\n",
    "            date_key = f\"{year}{MONTH_TO_02D[month]}{day}\"\n",
    "            \n",
    "            if date_s <= datetime.strptime(date_key, '%Y%m%d') < date_e:\n",
    "                dicti[datetime.strftime(date_s, '%Y_%m_%d')].append(file)\n",
    "        \n",
    "    for k, v in dicti.items():\n",
    "        arrL = []\n",
    "        for file in v:\n",
    "            ds = gdal.Open(file)\n",
    "            arr = ds.GetRasterBand(1).ReadAsArray()\n",
    "            arr[arr<=0] = np.nan\n",
    "            arr[arr>12] = np.nan\n",
    "            arrL.append(arr)\n",
    "        median_arr = np.nanmedian(np.dstack(arrL),axis=2)\n",
    "\n",
    "        if mask is not False:\n",
    "             outarr = median_arr * mask\n",
    "        else:\n",
    "             outarr = median_arr\n",
    "        makeTif_np_to_matching_tif(outarr, file, f'{outFold}median_9day/{ET_var}_{k}_median_ET.tif', 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e1156e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4 jobs will be processed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joblist = []\n",
    "\n",
    "for idx, et_var in enumerate(['Soil_func', 'Soil_calc', 'Canopy_func', 'Canopy_calc']):\n",
    "    for date_start, date_end in zip(starts, ends):\n",
    "        joblist.append([vrt_folder, et_var, date_start, date_end, date_reg2, evap_outFolder, mask_arr])\n",
    "\n",
    "\n",
    "print(f'\\n{len(joblist)} jobs will be processed\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61f5a092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Starting process, time:Tue, 21 Oct 2025 14:34:47\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Aldhani/users/potzschf/conda/envs/workhorse/lib/python3.12/site-packages/osgeo/gdal.py:311: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n",
      "/data/Aldhani/users/potzschf/conda/envs/workhorse/lib/python3.12/site-packages/osgeo/gdal.py:311: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n",
      "/data/Aldhani/users/potzschf/conda/envs/workhorse/lib/python3.12/site-packages/osgeo/gdal.py:311: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n",
      "/data/Aldhani/users/potzschf/conda/envs/workhorse/lib/python3.12/site-packages/osgeo/gdal.py:311: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_131985/245819635.py:23: RuntimeWarning: All-NaN slice encountered\n",
      "/tmp/ipykernel_131985/245819635.py:23: RuntimeWarning: All-NaN slice encountered\n",
      "/tmp/ipykernel_131985/245819635.py:23: RuntimeWarning: All-NaN slice encountered\n",
      "/tmp/ipykernel_131985/245819635.py:23: RuntimeWarning: All-NaN slice encountered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------\n",
      "--------------------------------------------------------\n",
      "start : Tue, 21 Oct 2025 14:34:47\n",
      "end: Tue, 21 Oct 2025 14:41:18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ncores = 4\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    starttime = time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Starting process, time:\" + starttime)\n",
    "    print(\"\")\n",
    "\n",
    "    Parallel(n_jobs=ncores)(delayed(make_ET_median)(job[0], job[1], job[2], job[3], job[4], job[5], job[6]) for job in joblist)\n",
    "\n",
    "    print(\"\")\n",
    "    endtime = time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"start : \" + starttime)\n",
    "    print(\"end: \" + endtime)\n",
    "    print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f1c9d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "medList = getFilelist(f'{evap_outFolder}median_9day/', '.tif')\n",
    "for date_start in starts:\n",
    "\n",
    "    dt = datetime.strftime(date_start, '%Y_%m_%d')\n",
    "    date_sub_calc = [med_arr for med_arr in medList if dt in med_arr and 'calc' in med_arr]\n",
    "    date_sub_func = [med_arr for med_arr in medList if dt in med_arr and 'func' in med_arr]\n",
    "\n",
    "    arrL = []\n",
    "    for pathi in date_sub_calc:\n",
    "        ds = gdal.Open(pathi)\n",
    "        arrL.append(ds.GetRasterBand(1).ReadAsArray())\n",
    "    arr_sum = np.nansum(np.dstack([arrL[0], arrL[1]]),axis=2)\n",
    "    arr_sum[arr_sum >= 10] = np.nan\n",
    "    # we cut off the highest values at the Polish border for now\n",
    "    cutOff = np.nanpercentile(arr_sum, [99.9])[0]\n",
    "    arr_sum[arr_sum > cutOff] = np.nan\n",
    "    makeTif_np_to_matching_tif(arr_sum, pathi, f'{evap_outFolder}median_9day/ET_sum/{dt}_median_ET.tif', 0)\n",
    "\n",
    "\n",
    "    # arrL = []\n",
    "    # for pathi in date_sub_func:\n",
    "    #     ds = gdal.Open(pathi)\n",
    "    #     arrL.append(ds.GetRasterBand(1).ReadAsArray())\n",
    "    # arr_sum = np.nansum(np.dstack([arrL[0], arrL[1]]), axis=2)\n",
    "    # arr_sum[arr_sum >= 10] = np.nan\n",
    "    # # we cut off the highest values at the Polish border for now\n",
    "    # cutOff = np.nanpercentile(arr_sum, [99.9])[0]\n",
    "    # arr_sum[arr_sum > cutOff] = np.nan\n",
    "\n",
    "    # makeTif_np_to_matching_tif(arr_sum, pathi, f'{evap_outFolder}median_9day/ET_sum/{date_list[4]}_median_func_ET.tif', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bbc713",
   "metadata": {},
   "outputs": [],
   "source": [
    "filesNames = ['Soil_func', 'Soil_calc', 'Canopy_func', 'Canopy_calc']\n",
    "\n",
    "\n",
    "for fname in filesNames:\n",
    "    files = getFilelist(f'{vrt_folder}{fname}/', '.vrt')\n",
    "    \n",
    "    dicti = defaultdict(list)\n",
    "    \n",
    "    for start_d, end_d in zip(starts, ends):\n",
    "    \n",
    "        for file in files:\n",
    "            m = date_reg2.search(file).group()[:-1]\n",
    "            year, month, day = m.split('_')\n",
    "            day = day.zfill(2)  # \"1\" → \"01\", \"13\" → \"13\"\n",
    "            date_key = f\"{year}{MONTH_TO_02D[month]}{day}\"\n",
    "            \n",
    "            if start_d <= datetime.strptime(date_key, '%Y%m%d') < end_d:\n",
    "                dicti[datetime.strftime(start_d, '%Y%m%d')].append(file)\n",
    "\n",
    "    for k, v in dicti.items():\n",
    "        arrL = []\n",
    "        for file in v:\n",
    "            ds = gdal.Open(file)\n",
    "            arr = ds.GetRasterBand(1).ReadAsArray()\n",
    "            arr[arr<=0] = np.nan\n",
    "            arr[arr>12] = np.nan\n",
    "            arrL.append(arr)\n",
    "        median_arr = np.nanmedian(np.dstack(arrL),axis=2)\n",
    "\n",
    "        makeTif_np_to_matching_tif(median_arr * mask_arr, file, f'{evap_outFolder}median_9day/{fname}_{k}_median_ET.tif', 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079bdcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a raster of mask with the dimensions of the vrts\n",
    "\n",
    "mask_path = '/data/Aldhani/eoagritwin/fields/Auxiliary/grid_search/Brandenburg/quick_n_dirty/Fields_polygons.shp'\n",
    "dummy_path = getFilelist(vrt_folder, '.vrt', deep=True)[0]\n",
    "mask_raster_path = '/data/Aldhani/eoagritwin/fields/Auxiliary/grid_search/Brandenburg/quick_n_dirty/Fields_as_mask_pixel_20.tif'\n",
    "\n",
    "\n",
    "aoi_ds = gdal.Open(dummy_path)\n",
    "geo_transform = aoi_ds.GetGeoTransform()\n",
    "projection = aoi_ds.GetProjection()\n",
    "x_res = aoi_ds.RasterXSize\n",
    "y_res = aoi_ds.RasterYSize\n",
    "\n",
    "# === Create empty target raster ===\n",
    "target_ds = gdal.GetDriverByName('GTiff').Create(\n",
    "    mask_raster_path,\n",
    "    x_res,\n",
    "    y_res,\n",
    "    1,                     # one band\n",
    "    gdal.GDT_Int32\n",
    ")\n",
    "target_ds.SetGeoTransform(geo_transform)\n",
    "target_ds.SetProjection(projection)\n",
    "\n",
    "# === Open shapefile ===\n",
    "shp_ds = ogr.Open(mask_path)\n",
    "layer = shp_ds.GetLayer()\n",
    "\n",
    "# === Rasterize ===\n",
    "# Use attribute \"FieldID\" from your shapefile for pixel values\n",
    "gdal.RasterizeLayer(\n",
    "    target_ds,\n",
    "    [1],             # which band(s) to burn into\n",
    "    layer,\n",
    "    options=[\"ATTRIBUTE=FieldID\"]\n",
    ")\n",
    "\n",
    "# === Clean up ===\n",
    "target_ds.FlushCache()\n",
    "target_ds = None\n",
    "shp_ds = None\n",
    "aoi_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb289b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
