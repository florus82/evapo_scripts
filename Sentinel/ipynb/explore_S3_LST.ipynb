{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/potzschf/repos/')\n",
    "from helperToolz.helpsters import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(getFilelist('/data/Aldhani/eoagritwin/et/Sentinel3/raw_LST', '.nc'))\n",
    "year = 2019\n",
    "# get a subset of files for that year\n",
    "yearFiles = [file for file in files if int(file.split('/')[-1].split('_')[-1][0:4]) == year]\n",
    "\n",
    "# make dictionary that stores .tif names and related accDates to easier search to close observations with S2 data\n",
    "lookUp = {}\n",
    "\n",
    "int_to_Month = {\n",
    "    '01': 'January',\n",
    "    '02': 'February',\n",
    "    '03': 'March',\n",
    "    '04': 'April',\n",
    "    '05': 'May',\n",
    "    '06': 'June',\n",
    "    '07': 'July',\n",
    "    '08': 'August',\n",
    "    '09': 'September',\n",
    "    '10': 'October',\n",
    "    '11': 'November',\n",
    "    '12': 'December'\n",
    "}\n",
    "\n",
    "# create a maks for germany\n",
    "mask = makeGermanyMaskforNC('/data/Aldhani/eoagritwin/misc/gadm41_DEU_shp/gadm41_DEU_0.shp', yearFiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/Aldhani/eoagritwin/et/Sentinel3/raw_LST/Germany_2019-01.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/raw_LST/Germany_2019-02.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/raw_LST/Germany_2019-03.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/raw_LST/Germany_2019-04.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/raw_LST/Germany_2019-05.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/raw_LST/Germany_2019-06.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/raw_LST/Germany_2019-07.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/raw_LST/Germany_2019-08.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/raw_LST/Germany_2019-09.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/raw_LST/Germany_2019-10.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/raw_LST/Germany_2019-11.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/raw_LST/Germany_2019-12.nc\n"
     ]
    }
   ],
   "source": [
    "# set storPath for exported tiffs\n",
    "storPath = '/data/Aldhani/eoagritwin/et/Sentinel3/tiffs/'\n",
    "LST_path = f'{storPath}LST/daily_observations_all/{year}/'\n",
    "Time_path = f'{storPath}Acq_time/{year}/'\n",
    "monthly_composites_path = f'{storPath}LST/monthly_composites/{year}/'\n",
    "# ensure that directories exist\n",
    "[os.makedirs(dir_path, exist_ok=True) for dir_path in [LST_path, Time_path, monthly_composites_path]]\n",
    "\n",
    "yearCont = []# for collecting number of observations per year\n",
    "\n",
    "# loop over files and export to .tif at Path locations\n",
    "for i, file in enumerate(yearFiles):\n",
    "    print(file)\n",
    "    if i != 6:\n",
    "        continue\n",
    "    accDateTimes = getAllDatesS3(file) # possible to take annual subset if entire files list would be passed here\n",
    "    convertNCtoTIF(file, '/data/Aldhani/eoagritwin/et/Sentinel3/temp/', file.split('/')[-1].split('.')[0] + '.tif', accDateTimes, False, True)\n",
    "\n",
    "    # dat = getDataFromNC_LST(file)\n",
    "    # monthCont = [] # for collecting number of observations per month\n",
    "    # dailyCont = [] # for collecting number of observations per day\n",
    "    # dailyVals_median = [] # for collection the actual daily LST values (daily median)\n",
    "    # dailyVals_mean = [] # for collection the actual daily LST values (daily mean)\n",
    "    # dailyVals_max = [] # for collection the actual daily LST values (daily mean)\n",
    "    # bnames = []\n",
    "    # df = pd.Series(accDateTimes)\n",
    "    # counts_per_day = df.dt.floor(\"D\").value_counts().sort_index()\n",
    "    # # vectors for indexing over days\n",
    "    # cumulative_day_counts_end = np.asarray(np.cumsum(counts_per_day))\n",
    "    # cumulative_day_counts_start = np.insert(cumulative_day_counts_end, 0 ,0)\n",
    "\n",
    "    # # get accquisition times ready for aggregation and export\n",
    "    # time_cube = np.full(dat.shape, 0, dtype='uint64')\n",
    "    # for i in range(len(df)):\n",
    "    #     mask = ~np.isnan(dat[:,:,i])\n",
    "    #     time_cube[:,:,i][mask] = int(pd.Timestamp(df[i]).strftime('%Y%m%d%H%M%S'))#df[i].timestamp() #needed conversion as tif export won't work with datetimeobject     \n",
    "    #     dailyTimeDates_max = []\n",
    "#     ################################################ gives monthly min, max, median composites\n",
    "#     # aggreagate by median\n",
    "#     # stack_list = [\n",
    "#     #     np.nanmedian(dat[:, :, start:end], axis=2)\n",
    "#     #     for start, end in zip(cumulative_day_counts_start[:-1], cumulative_day_counts_end)\n",
    "#     # ] \n",
    "#     # fin_block = np.dstack(stack_list)\n",
    "\n",
    "    MM = int_to_Month[file.rsplit('-', maxsplit=1)[-1].split('.')[0]]\n",
    "#     # bands = [f'{MM}_Day_{b+1}' for b in range(fin_block.shape[2])]\n",
    "#     # fin_block = fin_block * mask[:, :, np.newaxis]\n",
    "#     # fin_block[fin_block == 0] = np.nan\n",
    "#     # exportNCarrayDerivatesComp(file, monthly_composites_path, f'Germany_{year}_{MM}_mean.tif', bands, fin_block)\n",
    "\n",
    "#     # # aggreagate by min\n",
    "#     # stack_list = [\n",
    "#     #     np.nanmin(dat[:, :, start:end], axis=2)\n",
    "#     #     for start, end in zip(cumulative_day_counts_start[:-1], cumulative_day_counts_end)\n",
    "#     # ] \n",
    "#     # fin_block = np.dstack(stack_list)\n",
    "#     # fin_block = fin_block * mask[:, :, np.newaxis]\n",
    "#     # fin_block[fin_block == 0] = np.nan\n",
    "#     # exportNCarrayDerivatesComp(file, monthly_composites_path, f'Germany_{year}_{MM}_min.tif', bands, fin_block)\n",
    "\n",
    "#     # # aggreagate by max\n",
    "#     # stack_list = [\n",
    "\n",
    "#     #     np.nanmax(dat[:, :, start:end], axis=2)\n",
    "#     #     for start, end in zip(cumulative_day_counts_start[:-1], cumulative_day_counts_end)\n",
    "#     # ] \n",
    "#     # fin_block = np.dstack(stack_list)\n",
    "#     # fin_block = fin_block * mask[:, :, np.newaxis]\n",
    "#     # fin_block[fin_block == 0] = np.nan\n",
    "#     # exportNCarrayDerivatesComp(file, monthly_composites_path, f'Germany_{year}_{MM}_max.tif', bands, fin_block)\n",
    "\n",
    "#     ########################################################### creates metadata raster\n",
    "    # for l in range(len(counts_per_day)):\n",
    "    #     # number of observations per month\n",
    "    #     monthCont.append(np.any(~np.isnan(dat[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]]),axis=2)) # minimum dail obs\n",
    "        \n",
    "    #     # collect the dates to use as bandnames for exported tif stacks\n",
    "    #     bnames.append(str(counts_per_day.index[l].date()))\n",
    "\n",
    "    #     # collect number of observations per day ( count only one per day!)\n",
    "    #     dailyCont.append(np.sum(~np.isnan(dat[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]]),axis=2))\n",
    "        \n",
    "    #     # collect actual LST values\n",
    "    #     # dailyVals_median.append(np.nanmedian(dat[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]], axis = 2))\n",
    "    #     # dailyVals_mean.append(np.nanmean(dat[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]], axis = 2))\n",
    "    #     dailyVals_max.append(np.nanmax(dat[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]], axis = 2))\n",
    "\n",
    "    #     # collect the acquisition time and dates in timestamp format\n",
    "    #     dailyTimeDates_max.append(getValsatMaxIndex(dat[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]],\n",
    "    #                                                 time_cube[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]]))\n",
    "   \n",
    "#     # # export daily values\n",
    "#     # exportNCarrayDerivatesInt(file, LST_path, f'Daily_LST_means_{year}_{MM}.tif', bnames, np.dstack(dailyVals_mean), make_uint16=False, numberOfBands=len(dailyVals_mean))\n",
    "#     # exportNCarrayDerivatesInt(file, LST_path, f'Daily_LST_medians_{year}_{MM}.tif', bnames, np.dstack(dailyVals_median), make_uint16=False, numberOfBands=len(dailyVals_median))\n",
    "#     exportNCarrayDerivatesInt(file, LST_path, f'Daily_LST_max_{year}_{MM}.tif', bnames, np.dstack(dailyVals_max), make_uint16=False, numberOfBands=len(dailyVals_max))\n",
    "#     exportNCarrayDerivatesInt(file, Time_path, f'Daily_Time_max_{year}_{MM}.tif', bnames, np.dstack(dailyTimeDates_max), datType=gdal.GDT_UInt64, numberOfBands=len(dailyTimeDates_max), noData=0)\n",
    "#     # # export day counts\n",
    "#     # exportNCarrayDerivatesInt(file, storPath + 'Analytics/Count_obs_per_day/', f'Daily_obs_for_{year}_{MM}.tif', bnames, np.dstack(dailyCont), True, numberOfBands=len(dailyCont))\n",
    "#     # # export month counts\n",
    "#     # exportNCarrayDerivatesInt(file, storPath + 'Analytics/Count_obs_per_month/', f'Monthly_Min_DailyObs_{('_').join(file.split('_')[-1].split('-')[:2])}.tif', 'monthly_sum_of_daily_obs', np.nansum(np.dstack((monthCont)), axis = 2), True)\n",
    "    \n",
    "#     # # collect number of observations per year ( count only one per day!)\n",
    "#     # yearCont.append(np.nansum(np.dstack((monthCont)), axis = 2))\n",
    "\n",
    "# # # export year counts\n",
    "# # exportNCarrayDerivatesInt(file, storPath + 'Analytics/Count_obs_per_year/', f'Annual_Min_DailyObs_{file.split('_')[-1].split('-')[0]}.tif', 'annual_sum_of_daily_obs', np.nansum(np.dstack((yearCont)), axis = 2), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(getFilelist('/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw', '.nc'))\n",
    "year = 2019\n",
    "# get a subset of files for that year\n",
    "yearFiles = [file for file in files if int(file.split('/')[-1].split('_')[-1][0:4]) == year]\n",
    "\n",
    "# make dictionary that stores .tif names and related accDates to easier search to close observations with S2 data\n",
    "lookUp = {}\n",
    "\n",
    "int_to_Month = {\n",
    "    '01': 'January',\n",
    "    '02': 'February',\n",
    "    '03': 'March',\n",
    "    '04': 'April',\n",
    "    '05': 'May',\n",
    "    '06': 'June',\n",
    "    '07': 'July',\n",
    "    '08': 'August',\n",
    "    '09': 'September',\n",
    "    '10': 'October',\n",
    "    '11': 'November',\n",
    "    '12': 'December'\n",
    "}\n",
    "\n",
    "# create a maks for germany\n",
    "mask = makeGermanyMaskforNC('/data/Aldhani/eoagritwin/misc/gadm41_DEU_shp/gadm41_DEU_0.shp', yearFiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-01-01.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-01-15.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-02-01.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-02-15.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-03-01.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-03-15.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-04-01.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-04-15.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-05-01.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-05-15.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-06-01.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-06-15.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-07-01.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-07-15.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-08-01.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-08-15.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-09-01.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-09-15.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-10-01.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-10-15.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-11-01.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-11-15.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-12-01.nc\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/VZA/raw/Germany_2019-12-15.nc\n"
     ]
    }
   ],
   "source": [
    "# loop over files and export to .tif at Path locations\n",
    "for i, file in enumerate(yearFiles):\n",
    "    print(file)\n",
    "    if i != 12:\n",
    "        continue\n",
    "    accDateTimes = getAllDatesS3(file) # possible to take annual subset if entire files list would be passed here\n",
    "    convertNCtoTIF(file, '/data/Aldhani/eoagritwin/et/Sentinel3/temp/VZA/', f\"{file.split('/')[-1].split('.')[0]}.tif\",\n",
    "                   accDateTimes, False, explode=True, LST=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
