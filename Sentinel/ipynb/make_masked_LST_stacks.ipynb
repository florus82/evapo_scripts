{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0046117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/potzschf/repos/')\n",
    "from helperToolz.helpsters import *\n",
    "from helperToolz.guzinski import *\n",
    "from helperToolz.dicts_and_lists import INT_TO_MONTH\n",
    "\n",
    "# set storPath for exported tiffs\n",
    "LST_stor_Path_minVZA = '/data/Aldhani/eoagritwin/et/Sentinel3/LST/LST_values/LST_composites/minVZA/'\n",
    "LST_stor_Path_maxLST = '/data/Aldhani/eoagritwin/et/Sentinel3/LST/LST_values/LST_composites/maxLST/'\n",
    "LST_stor_Path_top3 = '/data/Aldhani/eoagritwin/et/Sentinel3/LST/LST_values/LST_composites/top3/'\n",
    "\n",
    "LST_path = '/data/Aldhani/eoagritwin/et/Sentinel3/raw_LST/'\n",
    "VZA_path = '/data/Aldhani/eoagritwin/et/Sentinel3/VZA/monthly_tiff_values/'\n",
    "VAA_path = '/data/Aldhani/eoagritwin/et/Sentinel3/VAA/monthly_tiff_values/'\n",
    "\n",
    "VZA_stor_Path_minVZA = '/data/Aldhani/eoagritwin/et/Sentinel3/VZA/comp/minVZA/'\n",
    "VZA_stor_Path_maxLST = '/data/Aldhani/eoagritwin/et/Sentinel3/VZA/comp/maxLST/'\n",
    "VZA_stor_Path_top3 = '/data/Aldhani/eoagritwin/et/Sentinel3/VZA/comp/top3/'\n",
    "\n",
    "VAA_stor_Path_minVZA = '/data/Aldhani/eoagritwin/et/Sentinel3/VZA/comp/minVZA/'\n",
    "VAA_stor_Path_maxLST = '/data/Aldhani/eoagritwin/et/Sentinel3/VZA/comp/maxLST/'\n",
    "VAA_stor_Path_top3 = '/data/Aldhani/eoagritwin/et/Sentinel3/VZA/comp/top3/'\n",
    "\n",
    "AcqTime_stor_path = '/data/Aldhani/eoagritwin/et/Sentinel3/LST/LST_values/Acq_time/int_format/'\n",
    "AirTemp_path = '/data/Aldhani/eoagritwin/et/Auxiliary/ERA5/tiff/low_res/2m_temperature/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae437e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Aldhani/users/potzschf/conda/envs/workhorse/lib/python3.12/site-packages/osgeo/ogr.py:601: FutureWarning: Neither ogr.UseExceptions() nor ogr.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# # for year in [2019]:\n",
    "year = 2019\n",
    "# set storage paths to the current year\n",
    "LST_stor_Path_minVZA_year = f'{LST_stor_Path_minVZA}{year}/'\n",
    "LST_stor_Path_maxLST_year = f'{LST_stor_Path_maxLST}{year}/' \n",
    "LST_stor_Path_top3_year =  f'{LST_stor_Path_top3}{year}/' \n",
    "\n",
    "VZA_stor_Path_minVZA_year = f'{VZA_stor_Path_minVZA}{year}/'\n",
    "VZA_stor_Path_maxLST_year = f'{VZA_stor_Path_maxLST}{year}/'\n",
    "VZA_stor_Path_top3_year = f'{VZA_stor_Path_top3}{year}/' \n",
    "\n",
    "VZA_stor_Path_minVZA_year = f'{VZA_stor_Path_minVZA}{year}/'\n",
    "VZA_stor_Path_maxLST_year = f'{VZA_stor_Path_maxLST}{year}/'\n",
    "VZA_stor_Path_top3_year = f'{VZA_stor_Path_top3}{year}/' \n",
    "\n",
    "AcqTime_stor_path_year = f'{AcqTime_stor_path}{year}/'\n",
    "\n",
    "# make paths if they don't exist\n",
    "[os.makedirs(path, exist_ok=True) for path in [LST_stor_Path_minVZA_year, LST_stor_Path_maxLST_year, LST_stor_Path_top3_year,\n",
    "                                                AcqTime_stor_path_year, VZA_stor_Path_maxLST_year, VZA_stor_Path_minVZA_year, VZA_stor_Path_top3_year]]\n",
    "\n",
    "# get a temporal subset of LST, VZA and air temp files for that year\n",
    "files = sorted(getFilelist(LST_path, '.nc'))\n",
    "yearFiles_LST = [file for file in sorted(getFilelist(LST_path, '.nc')) if int(file.split('/')[-1].split('_')[-1][0:4]) == year]\n",
    "yearFiles_VZA = getFilelist(f'{VZA_path}{year}/', 'tif')\n",
    "yearFiles_2mT = getFilelist(f'{AirTemp_path}{year}', '.tif')\n",
    "\n",
    "mask = makeGermanyMaskforNC('/data/Aldhani/eoagritwin/misc/gadm41_DEU_shp/gadm41_DEU_0.shp', yearFiles_LST[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8044169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "month = '07'\n",
    "# subset LST to month and get acquisition time and calculate observations per day\n",
    "file_LST = [yearfile_LST for yearfile_LST in yearFiles_LST if f'{month}.nc' == yearfile_LST.split('-')[-1]][0]\n",
    "accDateTimes = getAllDatesS3(file_LST) \n",
    "df = pd.Series(accDateTimes)\n",
    "counts_per_day = df.dt.floor(\"D\").value_counts().sort_index()\n",
    "# make iterables from counts per day that catch starting and ending indices to subset all obs per day\n",
    "cumulative_day_counts_end = np.asarray(np.cumsum(counts_per_day))\n",
    "cumulative_day_counts_start = np.insert(cumulative_day_counts_end, 0 ,0)\n",
    "\n",
    "# load data (/all observations for that month)\n",
    "dat_LST = getDataFromNC_LST(file_LST)\n",
    "\n",
    "# apply the temperature threshold\n",
    "dat_LST[dat_LST<273.15] = np.nan # LST_MASKING check!\n",
    "\n",
    "# get VZA stack and 2m airtemperature mask\n",
    "file_VZA = [yearfile_VZA for yearfile_VZA in yearFiles_VZA if f'{month}.tif' == yearfile_VZA.split('_')[-1]][0]\n",
    "dat_VZA = stackReader(file_VZA)\n",
    "\n",
    "# sanity check\n",
    "if (dat_LST.shape == dat_VZA.shape):\n",
    "\n",
    "    # check air temperature (2m ERA5)\n",
    "    file_2mT = [yearFile_2mT for yearFile_2mT in yearFiles_2mT if f'{INT_TO_MONTH[month]}.tif' == yearFile_2mT.split('_')[-1]][0]\n",
    "    dat_2mT, time_2mT = stackReader(file_2mT, bands=True)\n",
    "\n",
    "    #### get ERA5 AirTemp (interpolated from both modelled values that are closest to LST)\n",
    "    bands_low = []\n",
    "    minutes = [] # get the minutes to interpolate ERA5 temp values to the exact minute of LST acquisition\n",
    "    for accDT in accDateTimes: # search for each LST observation\n",
    "        for count, air_time in enumerate(time_2mT): # the two neighbouting ERA5 air temp values\n",
    "            if accDT.astype('datetime64[h]')== pd.Timestamp(air_time): # this will get the hourly value before the acquisition\n",
    "                bands_low.append(count)\n",
    "                minutes.append(pd.Timestamp(accDT).minute)\n",
    "    bands_up = [band + 1 for band in bands_low]# this get the hourly value after the acquisition\n",
    "    \n",
    "    # interpolate to the minute of observation\n",
    "    air_temp_intpol = dat_2mT[:,:,bands_low] - (dat_2mT[:,:,bands_low] - dat_2mT[:,:,bands_up]) * (np.array(minutes, dtype=np.float32) / 60).reshape(1,1,-1) # add one dimension for broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aa2a326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(893, 1083, 486)\n",
      "(893, 1083, 744)\n",
      "(893, 1083, 486)\n",
      "486\n",
      "/data/Aldhani/eoagritwin/et/Sentinel3/raw_LST/Germany_2019-07.nc\n"
     ]
    }
   ],
   "source": [
    "print(dat_LST.shape)\n",
    "print(dat_2mT.shape)\n",
    "print(air_temp_intpol.shape)\n",
    "print(len(accDateTimes))\n",
    "print(file_LST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ab5702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over files and export to .tif at Path locations\n",
    "for month in [f'{i:02d}' for i in range(1,13)]:\n",
    "    if month != '07':\n",
    "        continue\n",
    "    if growingSeasonChecker(int(month)):\n",
    "        print(month)\n",
    "        # subset LST to month and get acquisition time and calculate observations per day\n",
    "        file_LST = [yearfile_LST for yearfile_LST in yearFiles_LST if f'{month}.nc' == yearfile_LST.split('-')[-1]][0]\n",
    "        accDateTimes = getAllDatesS3(file_LST) \n",
    "        df = pd.Series(accDateTimes)\n",
    "        counts_per_day = df.dt.floor(\"D\").value_counts().sort_index()\n",
    "        # make iterables from counts per day that catch starting and ending indices to subset all obs per day\n",
    "        cumulative_day_counts_end = np.asarray(np.cumsum(counts_per_day))\n",
    "        cumulative_day_counts_start = np.insert(cumulative_day_counts_end, 0 ,0)\n",
    "\n",
    "        # load data (/all observations for that month)\n",
    "        dat_LST = getDataFromNC_LST(file_LST)\n",
    "\n",
    "        # apply the temperature threshold\n",
    "        dat_LST[dat_LST<273.15] = np.nan # LST_MASKING check!\n",
    "\n",
    "        # get VZA stack and 2m airtemperature mask\n",
    "        file_VZA = [yearfile_VZA for yearfile_VZA in yearFiles_VZA if f'{month}.tif' == yearfile_VZA.split('_')[-1]][0]\n",
    "        dat_VZA = stackReader(file_VZA)\n",
    "\n",
    "        # sanity check\n",
    "        if (dat_LST.shape == dat_VZA.shape):\n",
    "    \n",
    "            # check air temperature (2m ERA5)\n",
    "            file_2mT = [yearFile_2mT for yearFile_2mT in yearFiles_2mT if f'{INT_TO_MONTH[month]}.tif' == yearFile_2mT.split('_')[-1]][0]\n",
    "            dat_2mT, time_2mT = stackReader(file_2mT, bands=True)\n",
    "\n",
    "            #### get ERA5 AirTemp (interpolated from both modelled values that are closest to LST)\n",
    "            bands_low = []\n",
    "            minutes = [] # get the minutes to interpolate ERA5 temp values to the exact minute of LST acquisition\n",
    "            for accDT in accDateTimes: # search for each LST observation\n",
    "                for count, air_time in enumerate(time_2mT): # the two neighbouting ERA5 air temp values\n",
    "                    if accDT.astype('datetime64[h]')== pd.Timestamp(air_time): # this will get the hourly value before the acquisition\n",
    "                        bands_low.append(count)\n",
    "                        minutes.append(pd.Timestamp(accDT).minute)\n",
    "            bands_up = [band + 1 for band in bands_low]# this get the hourly value after the acquisition\n",
    "            \n",
    "            # interpolate to the minute of observation\n",
    "            air_temp_intpol = dat_2mT[:,:,bands_low] - (dat_2mT[:,:,bands_low] - dat_2mT[:,:,bands_up]) * (np.array(minutes, dtype=np.float32) / 60).reshape(1,1,-1) # add one dimension for broadcasting\n",
    "\n",
    "            # apply air threshold\n",
    "            dat_LST = np.where((dat_LST - air_temp_intpol) < -2, np.nan, dat_LST)\n",
    "       \n",
    "            # now get composites (minVZA, maxLST, single scenes)\n",
    "            minVZA_LST = [] # collects 2D numpy arrays with masked LST values from minVZA compositiing\n",
    "            maxLST_LST = [] # collects 2D numpy arrays with masked LST values from maxLST compositiing\n",
    "            minVZA_VZA = [] # collects 2D numpy arrays with masked VZA values from minVZA compositiing\n",
    "            maxLST_VZA = [] # collects 2D numpy arrays with masked VZA values from maxLST compositiing\n",
    "\n",
    "            minACQL = [] # collect acquisition times of minVZA pixel\n",
    "            minACQL_read = [] # collect readable acquisition times of minVZA pixel\n",
    "            maxACQL = [] # collect acquisition times of maxLST pixel\n",
    "            maxACQL_read = [] # collect readable acquisition times of maxLST pixel \n",
    "\n",
    "            lst_order1, lst_order2, lst_order3 = [], [], [] # order1 holds most pixel within scene\n",
    "            vza_order1, vza_order2, vza_order3 = [], [], []\n",
    "            order1_time_int, order2_time_int, order3_time_int = [], [], []  \n",
    "            order1_time_read, order2_time_read, order3_time_read = [], [], []  \n",
    "            \n",
    "            doyL = [] # for band names when exporting\n",
    "\n",
    "            for l in range(len(counts_per_day)):\n",
    "\n",
    "                ################## LST values\n",
    "                # Select the slices for the day:\n",
    "                LST_slice = dat_LST[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]]  # shape (X,Y,Z)\n",
    "                VZA_slice = dat_VZA[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]]  # shape (X,Y,Z)\n",
    "\n",
    "                # Create mask where LST is valid and VZA < 45\n",
    "                valid_mask = (~np.isnan(LST_slice)) & (VZA_slice < 45)\n",
    "                #valid_mask_maxLST = (~np.isnan(LST_slice)) \n",
    "\n",
    "                # For each (x,y), set VZA/LST invalid points to a large number so they don't become min\n",
    "                vza_for_minVZA = np.where(valid_mask, VZA_slice, np.inf)  # shape (X,Y,Z)\n",
    "                lst_for_maxLST = np.where(valid_mask, LST_slice, -np.inf)  # shape (X,Y,Z)\n",
    "\n",
    "                # Find index of minimal VZA/max LST along axis=2 (time/bands) for each pixel\n",
    "                minVZA_idx = np.argmin(vza_for_minVZA, axis=2)  # shape (X,Y)\n",
    "                maxLST_idx = np.argmax(lst_for_maxLST, axis=2)  # shape (X,Y)\n",
    "\n",
    "                # Now use advanced indexing to get the corresponding LST values:\n",
    "                x_indices = np.arange(LST_slice.shape[0])[:, None]  # shape (X,1)\n",
    "                y_indices = np.arange(LST_slice.shape[1])[None, :]  # shape (1,Y)\n",
    "\n",
    "                # fill \n",
    "                #best_LST_minVZA = LST_slice[x_indices, y_indices, minVZA_idx]  # shape (X,Y)\n",
    "                best_LST_maxLST = LST_slice[x_indices, y_indices, maxLST_idx]  # shape (X,Y)\n",
    "                #best_VZA_minVZA = VZA_slice[x_indices, y_indices, minVZA_idx]  # shape (X,Y)\n",
    "                best_VZA_maxLST = VZA_slice[x_indices, y_indices, maxLST_idx]  # shape (X,Y)\n",
    "\n",
    "                # Take care of all invalid pixel that might have sneaked in through np.argmin\n",
    "                no_valid_points = ~np.any(valid_mask, axis=2)  # shape (X,Y)\n",
    "\n",
    "\n",
    "                # best_LST_minVZA[no_valid_points] = np.nan\n",
    "                best_LST_maxLST[no_valid_points] = np.nan\n",
    "\n",
    "                # best_VZA_minVZA[no_valid_points] = np.nan\n",
    "                best_VZA_maxLST[no_valid_points] = np.nan\n",
    "\n",
    "                # minVZA_LST.append(best_LST_minVZA * mask)\n",
    "                maxLST_LST.append(best_LST_maxLST * mask)\n",
    "\n",
    "                # minVZA_VZA.append(best_VZA_minVZA * mask)\n",
    "                maxLST_VZA.append(best_VZA_maxLST * mask)\n",
    "\n",
    "                doyL.append(f'DOY_{l+1}')\n",
    "\n",
    "                # ################# Time of observation of selected pixel --> needed for ERA5 stuff\n",
    "                time_slice = df[cumulative_day_counts_start[l]:cumulative_day_counts_end[l]].values\n",
    "                timestamp_array = np.tile(time_slice, dat_LST.shape[:2] + (1,))\n",
    "                acq_time = timestamp_array[x_indices, y_indices, minVZA_idx]  \n",
    "                acq_time_unix = acq_time.astype('datetime64[s]').astype(int) # convert back with pd.to_datetime(best_time_unix, unit='s')\n",
    "                acq_time_unix[no_valid_points] = 0 # use 0 as na for export\n",
    "                minACQL.append(acq_time_unix * mask)\n",
    "\n",
    "                acq_time = timestamp_array[x_indices, y_indices, maxLST_idx]  \n",
    "                acq_time_unix = acq_time.astype('datetime64[s]').astype(int) # convert back with pd.to_datetime(best_time_unix, unit='s')\n",
    "                acq_time_unix[no_valid_points] = 0 # use 0 as na for export\n",
    "                maxACQL.append(acq_time_unix * mask)\n",
    "\n",
    "                # and also as readable tiffs\n",
    "                datetimes = time_slice.astype('datetime64[m]').astype('O')\n",
    "                time_arr = np.array([int(dt.strftime(\"%H%M\")) for dt in datetimes])\n",
    "                timestamp_array_read = np.tile(time_arr, dat_LST.shape[:2] + (1,))\n",
    "\n",
    "                acq_time = timestamp_array_read[x_indices, y_indices, minVZA_idx]  \n",
    "                acq_time[no_valid_points] = 0 # use 0 as na for export\n",
    "                minACQL_read.append(acq_time * mask)\n",
    "\n",
    "                acq_time = timestamp_array_read[x_indices, y_indices, maxLST_idx]  \n",
    "                acq_time[no_valid_points] = 0 # use 0 as na for export\n",
    "                maxACQL_read.append(acq_time * mask)\n",
    "\n",
    "                # single scene insert\n",
    "                lst_order = [lst_order1, lst_order2, lst_order3]\n",
    "                vza_order = [vza_order1, vza_order2, vza_order3]\n",
    "                order_time_int = [order1_time_int, order2_time_int, order3_time_int]\n",
    "                order_time_read = [order1_time_read, order2_time_read, order3_time_read]\n",
    "\n",
    "                # search scenes with largest coverage\n",
    "                val_pixels, val_counter = [], []\n",
    "                for sdx, scence in enumerate(range(valid_mask.shape[2])):\n",
    "                    val_pixels.append(np.sum(valid_mask[:,:,sdx]))\n",
    "                    val_counter.append(sdx)\n",
    "                snums, val_ind = sortListwithOtherlist(val_pixels, val_counter, rev=True)\n",
    "\n",
    "                for ldx, vali in enumerate(val_ind[:3]):\n",
    "                    lst_order[ldx].append(LST_slice[:,:,vali] * mask)\n",
    "                    vza_order[ldx].append(VZA_slice[:,:,vali] * valid_mask[:,:,vali].astype(int) * mask)\n",
    "\n",
    "                    acq_time_unix = acq_time = np.where(valid_mask[:,:,vali], timestamp_array[:,:,vali], np.datetime64('1970-01-01T00:00:00')) \n",
    "                    order_time_int[ldx].append(acq_time.astype('datetime64[s]').astype(int) * mask)\n",
    "                    order_time_read[ldx].append(np.where(valid_mask[:,:,vali], timestamp_array_read[:,:,vali], 0) * mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd73152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get composites (minVZA, maxLST, single scenes)\n",
    "minVZA_LST = [] # collects 2D numpy arrays with masked LST values from minVZA compositiing\n",
    "maxLST_LST = [] # collects 2D numpy arrays with masked LST values from maxLST compositiing\n",
    "minVZA_VZA = [] # collects 2D numpy arrays with masked VZA values from minVZA compositiing\n",
    "maxLST_VZA = [] # collects 2D numpy arrays with masked VZA values from maxLST compositiing\n",
    "\n",
    "minACQL = [] # collect acquisition times of minVZA pixel\n",
    "minACQL_read = [] # collect readable acquisition times of minVZA pixel\n",
    "maxACQL = [] # collect acquisition times of maxLST pixel\n",
    "maxACQL_read = [] # collect readable acquisition times of maxLST pixel \n",
    "\n",
    "lst_order1, lst_order2, lst_order3 = [], [], [] # order1 holds most pixel within scene\n",
    "vza_order1, vza_order2, vza_order3 = [], [], []\n",
    "order1_time_int, order2_time_int, order3_time_int = [], [], []  \n",
    "order1_time_read, order2_time_read, order3_time_read = [], [], []  \n",
    "\n",
    "doyL = [] # for band names when exporting\n",
    "\n",
    "for l in range(len(counts_per_day)):\n",
    "\n",
    "    ################## LST values\n",
    "    # Select the slices for the day:\n",
    "    LST_slice = dat_LST[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]]  # shape (X,Y,Z)\n",
    "    VZA_slice = dat_VZA[:, :, cumulative_day_counts_start[l]:cumulative_day_counts_end[l]]  # shape (X,Y,Z)\n",
    "\n",
    "    # Create mask where LST is valid and VZA < 45\n",
    "    valid_mask = (~np.isnan(LST_slice)) & (VZA_slice < 45)\n",
    "    #valid_mask_maxLST = (~np.isnan(LST_slice)) \n",
    "\n",
    "    # For each (x,y), set VZA/LST invalid points to a large number so they don't become min\n",
    "    vza_for_minVZA = np.where(valid_mask, VZA_slice, np.inf)  # shape (X,Y,Z)\n",
    "    lst_for_maxLST = np.where(valid_mask, LST_slice, -np.inf)  # shape (X,Y,Z)\n",
    "\n",
    "    # Find index of minimal VZA/max LST along axis=2 (time/bands) for each pixel\n",
    "    minVZA_idx = np.argmin(vza_for_minVZA, axis=2)  # shape (X,Y)\n",
    "    maxLST_idx = np.argmax(lst_for_maxLST, axis=2)  # shape (X,Y)\n",
    "\n",
    "    # Now use advanced indexing to get the corresponding LST values:\n",
    "    x_indices = np.arange(LST_slice.shape[0])[:, None]  # shape (X,1)\n",
    "    y_indices = np.arange(LST_slice.shape[1])[None, :]  # shape (1,Y)\n",
    "\n",
    "    # fill \n",
    "    #best_LST_minVZA = LST_slice[x_indices, y_indices, minVZA_idx]  # shape (X,Y)\n",
    "    best_LST_maxLST = LST_slice[x_indices, y_indices, maxLST_idx]  # shape (X,Y)\n",
    "    #best_VZA_minVZA = VZA_slice[x_indices, y_indices, minVZA_idx]  # shape (X,Y)\n",
    "    best_VZA_maxLST = VZA_slice[x_indices, y_indices, maxLST_idx]  # shape (X,Y)\n",
    "\n",
    "    # Take care of all invalid pixel that might have sneaked in through np.argmin\n",
    "    no_valid_points = ~np.any(valid_mask, axis=2)  # shape (X,Y)\n",
    "\n",
    "\n",
    "    # best_LST_minVZA[no_valid_points] = np.nan\n",
    "    best_LST_maxLST[no_valid_points] = np.nan\n",
    "\n",
    "    # best_VZA_minVZA[no_valid_points] = np.nan\n",
    "    best_VZA_maxLST[no_valid_points] = np.nan\n",
    "\n",
    "    # minVZA_LST.append(best_LST_minVZA * mask)\n",
    "    maxLST_LST.append(best_LST_maxLST * mask)\n",
    "\n",
    "    # minVZA_VZA.append(best_VZA_minVZA * mask)\n",
    "    maxLST_VZA.append(best_VZA_maxLST * mask)\n",
    "\n",
    "    doyL.append(f'DOY_{l+1}')\n",
    "\n",
    "    # ################# Time of observation of selected pixel --> needed for ERA5 stuff\n",
    "    time_slice = df[cumulative_day_counts_start[l]:cumulative_day_counts_end[l]].values\n",
    "    timestamp_array = np.tile(time_slice, dat_LST.shape[:2] + (1,))\n",
    "    acq_time = timestamp_array[x_indices, y_indices, minVZA_idx]  \n",
    "    acq_time_unix = acq_time.astype('datetime64[s]').astype(int) # convert back with pd.to_datetime(best_time_unix, unit='s')\n",
    "    acq_time_unix[no_valid_points] = 0 # use 0 as na for export\n",
    "    minACQL.append(acq_time_unix * mask)\n",
    "\n",
    "    acq_time = timestamp_array[x_indices, y_indices, maxLST_idx]  \n",
    "    acq_time_unix = acq_time.astype('datetime64[s]').astype(int) # convert back with pd.to_datetime(best_time_unix, unit='s')\n",
    "    acq_time_unix[no_valid_points] = 0 # use 0 as na for export\n",
    "    maxACQL.append(acq_time_unix * mask)\n",
    "\n",
    "    # and also as readable tiffs\n",
    "    datetimes = time_slice.astype('datetime64[m]').astype('O')\n",
    "    time_arr = np.array([int(dt.strftime(\"%H%M\")) for dt in datetimes])\n",
    "    timestamp_array_read = np.tile(time_arr, dat_LST.shape[:2] + (1,))\n",
    "\n",
    "    acq_time = timestamp_array_read[x_indices, y_indices, minVZA_idx]  \n",
    "    acq_time[no_valid_points] = 0 # use 0 as na for export\n",
    "    minACQL_read.append(acq_time * mask)\n",
    "\n",
    "    acq_time = timestamp_array_read[x_indices, y_indices, maxLST_idx]  \n",
    "    acq_time[no_valid_points] = 0 # use 0 as na for export\n",
    "    maxACQL_read.append(acq_time * mask)\n",
    "\n",
    "    # single scene insert\n",
    "    lst_order = [lst_order1, lst_order2, lst_order3]\n",
    "    vza_order = [vza_order1, vza_order2, vza_order3]\n",
    "    order_time_int = [order1_time_int, order2_time_int, order3_time_int]\n",
    "    order_time_read = [order1_time_read, order2_time_read, order3_time_read]\n",
    "\n",
    "    # search scenes with largest coverage\n",
    "    val_pixels, val_counter = [], []\n",
    "    for sdx, scence in enumerate(range(valid_mask.shape[2])):\n",
    "        val_pixels.append(np.sum(valid_mask[:,:,sdx]))\n",
    "        val_counter.append(sdx)\n",
    "    snums, val_ind = sortListwithOtherlist(val_pixels, val_counter, rev=True)\n",
    "\n",
    "    for ldx, vali in enumerate(val_ind[:3]):\n",
    "        lst_order[ldx].append(LST_slice[:,:,vali] * mask * valid_mask[:,:,vali].astype(int))\n",
    "        vza_order[ldx].append(VZA_slice[:,:,vali] * mask * valid_mask[:,:,vali].astype(int))\n",
    "\n",
    "        acq_time_unix = acq_time = np.where(valid_mask[:,:,vali], timestamp_array[:,:,vali], np.datetime64('1970-01-01T00:00:00')) \n",
    "        order_time_int[ldx].append(acq_time.astype('datetime64[s]').astype(int) * mask)\n",
    "        order_time_read[ldx].append(np.where(valid_mask[:,:,vali], timestamp_array_read[:,:,vali], 0) * mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c452e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################# export minVZA LST composite\n",
    "# # LST masked\n",
    "# exportNCarrayDerivatesInt(file_LST, LST_stor_Path_minVZA_year, f'Daily_LST_minVZA_{year}_{INT_TO_MONTH[month]}.tif',\n",
    "#                           doyL, np.dstack(minVZA_LST), numberOfBands=len(minVZA_LST), noData=0)\n",
    "# # # VZA masked\n",
    "# exportNCarrayDerivatesInt(file_LST, VZA_stor_Path_minVZA_year, f'Daily_VZA_minVZA_{year}_{INT_TO_MONTH[month]}.tif',\n",
    "#                           doyL, np.dstack(minVZA_VZA), numberOfBands=len(minVZA_VZA), noData=0)\n",
    "# # time\n",
    "# exportNCarrayDerivatesInt(file_LST, AcqTime_stor_path_year, f'Daily_AcqTime_minVZA_{year}_{INT_TO_MONTH[month]}.tif',\n",
    "#                         doyL, np.dstack(minACQL), datType=gdal.GDT_Int64, numberOfBands=len(minACQL), noData=0)\n",
    "# exportNCarrayDerivatesInt(file_LST, AcqTime_stor_path_year, f'Daily_AcqTime_minVZA_{year}_{INT_TO_MONTH[month]}_readable.tif',\n",
    "#                         doyL, np.dstack(minACQL_read), datType=gdal.GDT_Int64, numberOfBands=len(minACQL), noData=0)\n",
    "\n",
    "\n",
    "\n",
    "# ################# export max LST composite\n",
    "# # LST masked\n",
    "# exportNCarrayDerivatesInt(file_LST, LST_stor_Path_maxLST_year, f'Daily_LST_maxLST_{year}_{INT_TO_MONTH[month]}.tif',\n",
    "#                             doyL, np.dstack(maxLST_LST), numberOfBands=len(maxLST_LST), noData=0)\n",
    "# # VZA masked\n",
    "# exportNCarrayDerivatesInt(file_LST, VZA_stor_Path_maxLST_year, f'Daily_VZA_maxLST_{year}_{INT_TO_MONTH[month]}.tif',\n",
    "#                             doyL, np.dstack(maxLST_VZA), numberOfBands=len(maxLST_VZA), noData=0)\n",
    "# # time\n",
    "# exportNCarrayDerivatesInt(file_LST, AcqTime_stor_path_year, f'Daily_AcqTime_maxLST_{year}_{INT_TO_MONTH[month]}.tif',\n",
    "#                         doyL, np.dstack(maxACQL), datType=gdal.GDT_Int64 ,numberOfBands=len(maxACQL), noData=0)\n",
    "# exportNCarrayDerivatesInt(file_LST, AcqTime_stor_path_year, f'Daily_AcqTime_maxLST_{year}_{INT_TO_MONTH[month]}_readable.tif',\n",
    "#                         doyL, np.dstack(maxACQL_read), datType=gdal.GDT_Int64 ,numberOfBands=len(maxACQL), noData=0)\n",
    "\n",
    "\n",
    "\n",
    "################# export rank \"composites\"\n",
    "for ord in range(3):\n",
    "    print(ord)\n",
    "    # LST\n",
    "    exportNCarrayDerivatesInt(file_LST, LST_stor_Path_top3_year, f'Daily_LST_order{ord + 1}_{year}_{INT_TO_MONTH[month]}.tif',\n",
    "                                doyL, np.dstack(lst_order[ord]), numberOfBands=len(lst_order[ord]), noData=0)\n",
    "    # VZA \n",
    "    exportNCarrayDerivatesInt(file_LST, VZA_stor_Path_top3_year, f'Daily_VZA_order{ord + 1}_{year}_{INT_TO_MONTH[month]}.tif',\n",
    "                                doyL, np.dstack(vza_order[ord]), numberOfBands=len(vza_order[ord]), noData=0)\n",
    "    # time\n",
    "    exportNCarrayDerivatesInt(file_LST, AcqTime_stor_path_year, f'Daily_AcqTime_order{ord + 1}_{year}_{INT_TO_MONTH[month]}.tif',\n",
    "                            doyL, np.dstack(order_time_int[ord]), datType=gdal.GDT_Int64 ,numberOfBands=len(order_time_int[ord]), noData=0)\n",
    "    exportNCarrayDerivatesInt(file_LST, AcqTime_stor_path_year, f'Daily_AcqTime_order{ord + 1}_{year}_{INT_TO_MONTH[month]}_readable.tif',\n",
    "                            doyL, np.dstack(order_time_read[ord]), datType=gdal.GDT_Int64 ,numberOfBands=len(order_time_read[ord]), noData=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3d3bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "else:\n",
    "    raise ValueError(f'S3 LST stack differs from VZA stack. Something is seriously wrong\\nLST:{dat_LST.shape} vs VZA{dat_VZA.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acabd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script make_masked_LST_stacks.ipynb --output /home/potzschf/repos/evapo_scripts/Sentinel/py/make_masked_LST_stacks.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workhorse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
